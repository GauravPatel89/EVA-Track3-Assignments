{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Session3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GauravPatel89/EVA-Track3-Assignments/blob/master/Session3/Session3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNyZv-Ec52ot",
        "colab_type": "text"
      },
      "source": [
        "# **Import Libraries and modules**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1l5dG2pl1rT",
        "colab_type": "text"
      },
      "source": [
        "Following block installs keras in current runtime\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3m3w1Cw49Zkt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://keras.io/\n",
        "!pip install -q keras\n",
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mysmkrBjl8b0",
        "colab_type": "text"
      },
      "source": [
        "Import the different packages to use in our code.\n",
        "\n",
        "Last line imports the MNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eso6UHE080D4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Add\n",
        "from keras.layers import Convolution2D, MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint,Callback\n",
        "\n",
        "from keras.datasets import mnist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zByEi95J86RD",
        "colab_type": "text"
      },
      "source": [
        "### Load pre-shuffled MNIST data into train and test sets\n",
        "\n",
        "Load MNIST dataset\n",
        "\n",
        "X_train - input training data\n",
        "\n",
        "y_train - training outputs corresponding to X_train\n",
        "\n",
        "X_test - input testing data\n",
        "\n",
        "y_test - testing outputs corresponding to X_test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eRM0QWN83PV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0V7k0loolLn",
        "colab_type": "text"
      },
      "source": [
        "Print the size of X_train.\n",
        "\n",
        "-Size of training set is 60000 images of size 28x28\n",
        "\n",
        "-Show first input training image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a4Be72j8-ZC",
        "colab_type": "code",
        "outputId": "be7688b3-afca-4a41-b70b-e5d9966a2a48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        }
      },
      "source": [
        "print (X_train.shape)\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.imshow(X_train[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f3e102a0b70>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evenqcZ0owVA",
        "colab_type": "text"
      },
      "source": [
        "Change the size of X_train and X_test from 60000x28x28 to 60000x28x28x1\n",
        "We are adding one more level of dimension to the input. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkmprriw9AnZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], 28, 28,1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYf5dktQo0T3",
        "colab_type": "text"
      },
      "source": [
        "Right now dataset loaded is of type uint8. Each pixel is integer of size 8 bit. We first convert training data to float of size 32 bits.\n",
        "\n",
        "To normalize the image data we divide whole data by 255 i.e. highest 8bit value for a pixel. After the division all the pixel values are between 0.0 and 1.0 float32."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2m4YS4E9CRh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-LmKcdSo532",
        "colab_type": "text"
      },
      "source": [
        "Print first 10 output classes for training dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Mn0vAYD9DvB",
        "colab_type": "code",
        "outputId": "bbd805a9-d28c-4cbb-9e96-20bcf4f09993",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_train[:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XwhiD4So9Gs",
        "colab_type": "text"
      },
      "source": [
        "Convert training and test output from simple class number to 'one hot encoding'. In this encoding we have as many bits as the number of classes.We have binary '1' for bit number corresponding to the class of the input. For other classes we have '0'.\n",
        "\n",
        "eg. '5' == '0 0 0 0 1 0 0 0 0 0' ; '10' == '0 0 0 0 0 0 0 0 0 1'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG8JiXR39FHC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert 1-dimensional class arrays to 10-dimensional class matrices\n",
        "Y_train = np_utils.to_categorical(y_train, 10)\n",
        "Y_test = np_utils.to_categorical(y_test, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYlFRvKS9HMB",
        "colab_type": "code",
        "outputId": "b2b990bd-0d3e-4bf4-981d-9da5e6bc4828",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "Y_train[:10]           # print the first 10 training outputs"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6F27EhE4hDrA",
        "colab_type": "text"
      },
      "source": [
        "Create Network model\n",
        "\n",
        "Convolution2D(n,k,k,'activation=aa') means convolution layer with **'n'** kernels of size **kxk** with activation model  **'aa'**.\n",
        "\n",
        "For the first layer we also provide input shape 28x28x1 as we have 28x28 image with single channel.\n",
        "\n",
        "\n",
        "MaxPooling2D(pool_size=(2, 2)) means maxPooling layer of size 2x2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osKqT73Q9JJB",
        "colab_type": "code",
        "outputId": "33df293e-77f7-479e-bc16-6d2c697efc08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "from keras.layers import Activation\n",
        "model = Sequential()\n",
        "\n",
        "                                                                              # input is 28x28x1         Receptive Field = 1x1\n",
        "model.add(Convolution2D(10, 3, 3, activation='relu', input_shape=(28,28,1)))  # Receptive Field = 3x3    i/p  = 28x28x1    kernel = (3x3x1)x10     o/p = 26x26x10\n",
        "model.add(Convolution2D(16, 3, 3, activation='relu'))                         # Receptive Field = 5x5    i/p  = 26x26x10   kernel = (3x3x10)x16    o/p = 24x24x16\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))                                     # Receptive Field = 10x10  i/p  = 24x24x16   MaxPooling of 2x2       o/p = 12x12x16\n",
        "model.add(Convolution2D(10, 1, activation='relu'))                            # Receptive Field = 10x10  i/p  = 12x12x16   kernel = (1x1x16)x10    o/p = 12x12x10\n",
        "model.add(Convolution2D(16, 3, 3, activation='relu'))                         # Receptive Field = 12x12  i/p  = 26x26x10   kernel = (3x3x10)x16    o/p = 10x10x16\n",
        "model.add(Convolution2D(24, 3, 3, activation='relu'))                         # Receptive Field = 14x14  i/p  = 10x10x16   kernel = (3x3x16)x24    o/p = 8x8x24\n",
        "model.add(Convolution2D(32, 3, 3, activation='relu'))                         # Receptive Field = 16x16  i/p  = 8x8x24     kernel = (3x3x24)x32    o/p = 6x6x32\n",
        "model.add(Convolution2D(16, 1, activation='relu'))                            # Receptive Field = 16x16  i/p  = 6x6x32     kernel = (1x1x32)x16    o/p = 6x6x16\n",
        "model.add(Convolution2D(19, 3, 3, activation='relu'))                         # Receptive Field = 18x18  i/p  = 6x6x16     kernel = (3x3x16)x19    o/p = 4x4x19\n",
        "model.add(Convolution2D(10, 4))                                               # Receptive Field = 21x21  i/p  = 6x6x19     kernel = (4x4x19)x10    o/p = 1x1x10\n",
        "model.add(Flatten())                                                          # flatten the 1x1x10 array into 10x1 array\n",
        "model.add(Activation('softmax'))                                              # apply the softmax function to obtain classification weights"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (3, 3), activation=\"relu\", input_shape=(28, 28, 1...)`\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\")`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\")`\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(24, (3, 3), activation=\"relu\")`\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\")`\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(19, (3, 3), activation=\"relu\")`\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzdAYg1k9K7Z",
        "colab_type": "code",
        "outputId": "4cfd3fa2-4774-4142-82dc-ca6564120189",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        }
      },
      "source": [
        "model.summary()    # Show model information like layers, outputs, number of parameters"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_10 (Conv2D)           (None, 26, 26, 10)        100       \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 24, 24, 16)        1456      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 12, 12, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 12, 12, 10)        170       \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 10, 10, 16)        1456      \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 8, 8, 24)          3480      \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 6, 6, 32)          6944      \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 6, 6, 16)          528       \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 4, 4, 19)          2755      \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 1, 1, 10)          3050      \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 19,939\n",
            "Trainable params: 19,939\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBPUvgXKS9jE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#following code is to stop the model training at some specified validaton accuracy\n",
        "class EarlyStoppingByAccuracy(Callback):\n",
        "    def __init__(self, monitor='accuracy', value=0.99, verbose=0):\n",
        "        super(Callback, self).__init__()\n",
        "        self.monitor = monitor\n",
        "        self.value = value\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        current = logs.get(self.monitor)\n",
        "        if current is None:\n",
        "            warnings.warn(\"Early stopping requires %s available!\" % self.monitor, RuntimeWarning)\n",
        "\n",
        "        if current >= self.value:\n",
        "            if self.verbose > 0:\n",
        "                print(\"Epoch %05d: early stopping THR\" % epoch)\n",
        "            self.model.stop_training = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zp6SuGrL9M3h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# compile the model by specifyinng the optimizer, loss function, and training metrics to be used\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics=['accuracy'])\n",
        "\n",
        "es = EarlyStoppingByAccuracy(monitor='val_acc', value=0.994, verbose=1)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xWoKhPY9Of5",
        "colab_type": "code",
        "outputId": "61e0e5c1-ce0a-4ea4-bf1a-7fca3f37cfe3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#start the model training with test sample validation after each epoch. Use batch size as 32 and number of epochs as 50\n",
        "#use callback to stop the model training once the validation accuracy reaches 0.994\n",
        "model.fit(X_train, Y_train,validation_data=(X_test, Y_test), batch_size=32, nb_epoch=50, verbose=1,callbacks=[es])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/50\n",
            "60000/60000 [==============================] - 22s 374us/step - loss: 0.2464 - acc: 0.9213 - val_loss: 0.0743 - val_acc: 0.9756\n",
            "Epoch 2/50\n",
            "60000/60000 [==============================] - 22s 363us/step - loss: 0.0763 - acc: 0.9766 - val_loss: 0.0580 - val_acc: 0.9836\n",
            "Epoch 3/50\n",
            "60000/60000 [==============================] - 22s 362us/step - loss: 0.0570 - acc: 0.9824 - val_loss: 0.0444 - val_acc: 0.9868\n",
            "Epoch 4/50\n",
            "60000/60000 [==============================] - 22s 364us/step - loss: 0.0467 - acc: 0.9858 - val_loss: 0.0712 - val_acc: 0.9791\n",
            "Epoch 5/50\n",
            "60000/60000 [==============================] - 22s 363us/step - loss: 0.0409 - acc: 0.9873 - val_loss: 0.0427 - val_acc: 0.9876\n",
            "Epoch 6/50\n",
            "60000/60000 [==============================] - 22s 363us/step - loss: 0.0339 - acc: 0.9892 - val_loss: 0.0473 - val_acc: 0.9858\n",
            "Epoch 7/50\n",
            "60000/60000 [==============================] - 22s 365us/step - loss: 0.0308 - acc: 0.9904 - val_loss: 0.0377 - val_acc: 0.9881\n",
            "Epoch 8/50\n",
            "60000/60000 [==============================] - 22s 364us/step - loss: 0.0268 - acc: 0.9913 - val_loss: 0.0409 - val_acc: 0.9864\n",
            "Epoch 9/50\n",
            "60000/60000 [==============================] - 22s 364us/step - loss: 0.0243 - acc: 0.9922 - val_loss: 0.0396 - val_acc: 0.9872\n",
            "Epoch 10/50\n",
            "60000/60000 [==============================] - 22s 365us/step - loss: 0.0228 - acc: 0.9927 - val_loss: 0.0382 - val_acc: 0.9873\n",
            "Epoch 11/50\n",
            "60000/60000 [==============================] - 22s 367us/step - loss: 0.0204 - acc: 0.9935 - val_loss: 0.0368 - val_acc: 0.9889\n",
            "Epoch 12/50\n",
            "60000/60000 [==============================] - 22s 372us/step - loss: 0.0193 - acc: 0.9937 - val_loss: 0.0348 - val_acc: 0.9889\n",
            "Epoch 13/50\n",
            "60000/60000 [==============================] - 22s 369us/step - loss: 0.0173 - acc: 0.9940 - val_loss: 0.0327 - val_acc: 0.9897\n",
            "Epoch 14/50\n",
            "60000/60000 [==============================] - 22s 369us/step - loss: 0.0153 - acc: 0.9948 - val_loss: 0.0386 - val_acc: 0.9888\n",
            "Epoch 15/50\n",
            "60000/60000 [==============================] - 22s 367us/step - loss: 0.0159 - acc: 0.9945 - val_loss: 0.0474 - val_acc: 0.9876\n",
            "Epoch 16/50\n",
            "60000/60000 [==============================] - 22s 369us/step - loss: 0.0150 - acc: 0.9949 - val_loss: 0.0375 - val_acc: 0.9904\n",
            "Epoch 17/50\n",
            "60000/60000 [==============================] - 22s 367us/step - loss: 0.0151 - acc: 0.9951 - val_loss: 0.0331 - val_acc: 0.9903\n",
            "Epoch 18/50\n",
            "60000/60000 [==============================] - 22s 365us/step - loss: 0.0132 - acc: 0.9958 - val_loss: 0.0360 - val_acc: 0.9913\n",
            "Epoch 19/50\n",
            "60000/60000 [==============================] - 22s 366us/step - loss: 0.0131 - acc: 0.9957 - val_loss: 0.0456 - val_acc: 0.9879\n",
            "Epoch 20/50\n",
            "60000/60000 [==============================] - 22s 368us/step - loss: 0.0121 - acc: 0.9965 - val_loss: 0.0399 - val_acc: 0.9905\n",
            "Epoch 21/50\n",
            "60000/60000 [==============================] - 22s 368us/step - loss: 0.0121 - acc: 0.9961 - val_loss: 0.0374 - val_acc: 0.9920\n",
            "Epoch 22/50\n",
            "60000/60000 [==============================] - 22s 366us/step - loss: 0.0107 - acc: 0.9965 - val_loss: 0.0533 - val_acc: 0.9878\n",
            "Epoch 23/50\n",
            "60000/60000 [==============================] - 22s 365us/step - loss: 0.0120 - acc: 0.9962 - val_loss: 0.0388 - val_acc: 0.9905\n",
            "Epoch 24/50\n",
            "60000/60000 [==============================] - 22s 365us/step - loss: 0.0108 - acc: 0.9965 - val_loss: 0.0408 - val_acc: 0.9912\n",
            "Epoch 25/50\n",
            "60000/60000 [==============================] - 22s 364us/step - loss: 0.0120 - acc: 0.9961 - val_loss: 0.0366 - val_acc: 0.9911\n",
            "Epoch 26/50\n",
            "60000/60000 [==============================] - 22s 373us/step - loss: 0.0097 - acc: 0.9969 - val_loss: 0.0331 - val_acc: 0.9907\n",
            "Epoch 27/50\n",
            "60000/60000 [==============================] - 22s 371us/step - loss: 0.0105 - acc: 0.9966 - val_loss: 0.0434 - val_acc: 0.9907\n",
            "Epoch 28/50\n",
            "60000/60000 [==============================] - 22s 367us/step - loss: 0.0102 - acc: 0.9967 - val_loss: 0.0457 - val_acc: 0.9898\n",
            "Epoch 29/50\n",
            "60000/60000 [==============================] - 22s 365us/step - loss: 0.0096 - acc: 0.9969 - val_loss: 0.0418 - val_acc: 0.9906\n",
            "Epoch 30/50\n",
            "60000/60000 [==============================] - 22s 366us/step - loss: 0.0092 - acc: 0.9972 - val_loss: 0.0463 - val_acc: 0.9894\n",
            "Epoch 31/50\n",
            "60000/60000 [==============================] - 22s 366us/step - loss: 0.0115 - acc: 0.9963 - val_loss: 0.0443 - val_acc: 0.9905\n",
            "Epoch 32/50\n",
            "60000/60000 [==============================] - 22s 367us/step - loss: 0.0087 - acc: 0.9972 - val_loss: 0.0476 - val_acc: 0.9896\n",
            "Epoch 33/50\n",
            "60000/60000 [==============================] - 22s 367us/step - loss: 0.0087 - acc: 0.9976 - val_loss: 0.0426 - val_acc: 0.9902\n",
            "Epoch 34/50\n",
            "60000/60000 [==============================] - 22s 368us/step - loss: 0.0088 - acc: 0.9973 - val_loss: 0.0468 - val_acc: 0.9893\n",
            "Epoch 35/50\n",
            "60000/60000 [==============================] - 22s 367us/step - loss: 0.0098 - acc: 0.9968 - val_loss: 0.0403 - val_acc: 0.9906\n",
            "Epoch 36/50\n",
            "60000/60000 [==============================] - 22s 366us/step - loss: 0.0065 - acc: 0.9981 - val_loss: 0.0745 - val_acc: 0.9864\n",
            "Epoch 37/50\n",
            "60000/60000 [==============================] - 22s 366us/step - loss: 0.0100 - acc: 0.9969 - val_loss: 0.0530 - val_acc: 0.9906\n",
            "Epoch 38/50\n",
            "60000/60000 [==============================] - 22s 366us/step - loss: 0.0081 - acc: 0.9976 - val_loss: 0.0509 - val_acc: 0.9905\n",
            "Epoch 39/50\n",
            "60000/60000 [==============================] - 22s 366us/step - loss: 0.0095 - acc: 0.9971 - val_loss: 0.0444 - val_acc: 0.9907\n",
            "Epoch 40/50\n",
            "60000/60000 [==============================] - 22s 371us/step - loss: 0.0084 - acc: 0.9975 - val_loss: 0.0485 - val_acc: 0.9900\n",
            "Epoch 41/50\n",
            "60000/60000 [==============================] - 22s 371us/step - loss: 0.0076 - acc: 0.9976 - val_loss: 0.0461 - val_acc: 0.9910\n",
            "Epoch 42/50\n",
            "60000/60000 [==============================] - 22s 366us/step - loss: 0.0101 - acc: 0.9970 - val_loss: 0.0435 - val_acc: 0.9902\n",
            "Epoch 43/50\n",
            "60000/60000 [==============================] - 22s 366us/step - loss: 0.0082 - acc: 0.9976 - val_loss: 0.0479 - val_acc: 0.9897\n",
            "Epoch 44/50\n",
            "60000/60000 [==============================] - 22s 368us/step - loss: 0.0077 - acc: 0.9975 - val_loss: 0.0472 - val_acc: 0.9899\n",
            "Epoch 45/50\n",
            "60000/60000 [==============================] - 22s 366us/step - loss: 0.0090 - acc: 0.9972 - val_loss: 0.0457 - val_acc: 0.9901\n",
            "Epoch 46/50\n",
            "60000/60000 [==============================] - 22s 370us/step - loss: 0.0066 - acc: 0.9980 - val_loss: 0.0437 - val_acc: 0.9911\n",
            "Epoch 47/50\n",
            "60000/60000 [==============================] - 22s 367us/step - loss: 0.0090 - acc: 0.9973 - val_loss: 0.0481 - val_acc: 0.9915\n",
            "Epoch 48/50\n",
            "60000/60000 [==============================] - 22s 367us/step - loss: 0.0077 - acc: 0.9978 - val_loss: 0.0556 - val_acc: 0.9892\n",
            "Epoch 49/50\n",
            "60000/60000 [==============================] - 22s 367us/step - loss: 0.0086 - acc: 0.9974 - val_loss: 0.0463 - val_acc: 0.9921\n",
            "Epoch 50/50\n",
            "60000/60000 [==============================] - 22s 366us/step - loss: 0.0056 - acc: 0.9983 - val_loss: 0.0512 - val_acc: 0.9906\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3e1025a898>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcHL2rUJIbBB",
        "colab_type": "text"
      },
      "source": [
        "I experimented with various model layers, batch size and number of epochs. Once I finalised this particular model, while training I observed that \n",
        "with smaller batch sizes, although training time was large model was reaching validation accuracy of as high as 99.2 % (epoch 21) in lesser epochs.\n",
        "Once that kind of accuracy is reached when I trained the network further with larger batch size and very large number of epochs, I could consistently achieve validation accuracy of 99.3% and more. Here in this file I could reach highest accuracy of 99.34% (Epoch 60). I could not improve the validation accuracy any further although the training accuracy reached 100 %. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFfRgIV9YGy8",
        "colab_type": "code",
        "outputId": "0d434842-1f30-4ba5-8c07-b6ef2ee5bdff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(X_train, Y_train,validation_data=(X_test, Y_test), batch_size=1024, nb_epoch=500, verbose=1,callbacks=[es])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/500\n",
            " 3072/60000 [>.............................] - ETA: 3s - loss: 0.0022 - acc: 0.9993"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "60000/60000 [==============================] - 3s 46us/step - loss: 0.0035 - acc: 0.9989 - val_loss: 0.0488 - val_acc: 0.9913\n",
            "Epoch 2/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 0.0012 - acc: 0.9997 - val_loss: 0.0474 - val_acc: 0.9914\n",
            "Epoch 3/500\n",
            "60000/60000 [==============================] - 3s 44us/step - loss: 7.3791e-04 - acc: 0.9999 - val_loss: 0.0469 - val_acc: 0.9917\n",
            "Epoch 4/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 5.8488e-04 - acc: 1.0000 - val_loss: 0.0466 - val_acc: 0.9915\n",
            "Epoch 5/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 5.1581e-04 - acc: 1.0000 - val_loss: 0.0465 - val_acc: 0.9918\n",
            "Epoch 6/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 4.7329e-04 - acc: 1.0000 - val_loss: 0.0464 - val_acc: 0.9918\n",
            "Epoch 7/500\n",
            "60000/60000 [==============================] - 3s 44us/step - loss: 4.4347e-04 - acc: 1.0000 - val_loss: 0.0464 - val_acc: 0.9918\n",
            "Epoch 8/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 4.2227e-04 - acc: 1.0000 - val_loss: 0.0464 - val_acc: 0.9919\n",
            "Epoch 9/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 4.0473e-04 - acc: 1.0000 - val_loss: 0.0464 - val_acc: 0.9918\n",
            "Epoch 10/500\n",
            "60000/60000 [==============================] - 3s 44us/step - loss: 3.9161e-04 - acc: 1.0000 - val_loss: 0.0464 - val_acc: 0.9919\n",
            "Epoch 11/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 3.8039e-04 - acc: 1.0000 - val_loss: 0.0464 - val_acc: 0.9920\n",
            "Epoch 12/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 3.7110e-04 - acc: 1.0000 - val_loss: 0.0464 - val_acc: 0.9922\n",
            "Epoch 13/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 3.6336e-04 - acc: 1.0000 - val_loss: 0.0465 - val_acc: 0.9922\n",
            "Epoch 14/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 3.5681e-04 - acc: 1.0000 - val_loss: 0.0465 - val_acc: 0.9922\n",
            "Epoch 15/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 3.5103e-04 - acc: 1.0000 - val_loss: 0.0465 - val_acc: 0.9923\n",
            "Epoch 16/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 3.4552e-04 - acc: 1.0000 - val_loss: 0.0465 - val_acc: 0.9924\n",
            "Epoch 17/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 3.4084e-04 - acc: 1.0000 - val_loss: 0.0466 - val_acc: 0.9924\n",
            "Epoch 18/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 3.3668e-04 - acc: 1.0000 - val_loss: 0.0466 - val_acc: 0.9924\n",
            "Epoch 19/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 3.3273e-04 - acc: 1.0000 - val_loss: 0.0466 - val_acc: 0.9926\n",
            "Epoch 20/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 3.2925e-04 - acc: 1.0000 - val_loss: 0.0467 - val_acc: 0.9926\n",
            "Epoch 21/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 3.2593e-04 - acc: 1.0000 - val_loss: 0.0467 - val_acc: 0.9926\n",
            "Epoch 22/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 3.2304e-04 - acc: 1.0000 - val_loss: 0.0468 - val_acc: 0.9926\n",
            "Epoch 23/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 3.2031e-04 - acc: 1.0000 - val_loss: 0.0468 - val_acc: 0.9927\n",
            "Epoch 24/500\n",
            "60000/60000 [==============================] - 3s 44us/step - loss: 3.1767e-04 - acc: 1.0000 - val_loss: 0.0469 - val_acc: 0.9927\n",
            "Epoch 25/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 3.1524e-04 - acc: 1.0000 - val_loss: 0.0469 - val_acc: 0.9927\n",
            "Epoch 26/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 3.1307e-04 - acc: 1.0000 - val_loss: 0.0470 - val_acc: 0.9927\n",
            "Epoch 27/500\n",
            "60000/60000 [==============================] - 3s 44us/step - loss: 3.1107e-04 - acc: 1.0000 - val_loss: 0.0470 - val_acc: 0.9927\n",
            "Epoch 28/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 3.0915e-04 - acc: 1.0000 - val_loss: 0.0471 - val_acc: 0.9928\n",
            "Epoch 29/500\n",
            "60000/60000 [==============================] - 3s 44us/step - loss: 3.0737e-04 - acc: 1.0000 - val_loss: 0.0471 - val_acc: 0.9928\n",
            "Epoch 30/500\n",
            "60000/60000 [==============================] - 3s 44us/step - loss: 3.0568e-04 - acc: 1.0000 - val_loss: 0.0472 - val_acc: 0.9928\n",
            "Epoch 31/500\n",
            "60000/60000 [==============================] - 3s 44us/step - loss: 3.0404e-04 - acc: 1.0000 - val_loss: 0.0472 - val_acc: 0.9928\n",
            "Epoch 32/500\n",
            "60000/60000 [==============================] - 3s 44us/step - loss: 3.0251e-04 - acc: 1.0000 - val_loss: 0.0473 - val_acc: 0.9929\n",
            "Epoch 33/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 3.0111e-04 - acc: 1.0000 - val_loss: 0.0473 - val_acc: 0.9929\n",
            "Epoch 34/500\n",
            "60000/60000 [==============================] - 3s 44us/step - loss: 2.9978e-04 - acc: 1.0000 - val_loss: 0.0474 - val_acc: 0.9929\n",
            "Epoch 35/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.9855e-04 - acc: 1.0000 - val_loss: 0.0474 - val_acc: 0.9929\n",
            "Epoch 36/500\n",
            "60000/60000 [==============================] - 3s 44us/step - loss: 2.9730e-04 - acc: 1.0000 - val_loss: 0.0475 - val_acc: 0.9929\n",
            "Epoch 37/500\n",
            "60000/60000 [==============================] - 3s 44us/step - loss: 2.9618e-04 - acc: 1.0000 - val_loss: 0.0476 - val_acc: 0.9931\n",
            "Epoch 38/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.9508e-04 - acc: 1.0000 - val_loss: 0.0476 - val_acc: 0.9931\n",
            "Epoch 39/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.9405e-04 - acc: 1.0000 - val_loss: 0.0477 - val_acc: 0.9931\n",
            "Epoch 40/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.9308e-04 - acc: 1.0000 - val_loss: 0.0477 - val_acc: 0.9931\n",
            "Epoch 41/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.9213e-04 - acc: 1.0000 - val_loss: 0.0478 - val_acc: 0.9932\n",
            "Epoch 42/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.9121e-04 - acc: 1.0000 - val_loss: 0.0478 - val_acc: 0.9932\n",
            "Epoch 43/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.9035e-04 - acc: 1.0000 - val_loss: 0.0479 - val_acc: 0.9932\n",
            "Epoch 44/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.8952e-04 - acc: 1.0000 - val_loss: 0.0479 - val_acc: 0.9932\n",
            "Epoch 45/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.8875e-04 - acc: 1.0000 - val_loss: 0.0480 - val_acc: 0.9932\n",
            "Epoch 46/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.8799e-04 - acc: 1.0000 - val_loss: 0.0480 - val_acc: 0.9932\n",
            "Epoch 47/500\n",
            "60000/60000 [==============================] - 3s 44us/step - loss: 2.8729e-04 - acc: 1.0000 - val_loss: 0.0481 - val_acc: 0.9931\n",
            "Epoch 48/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.8661e-04 - acc: 1.0000 - val_loss: 0.0481 - val_acc: 0.9931\n",
            "Epoch 49/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.8594e-04 - acc: 1.0000 - val_loss: 0.0482 - val_acc: 0.9931\n",
            "Epoch 50/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.8531e-04 - acc: 1.0000 - val_loss: 0.0482 - val_acc: 0.9931\n",
            "Epoch 51/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.8471e-04 - acc: 1.0000 - val_loss: 0.0483 - val_acc: 0.9931\n",
            "Epoch 52/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.8412e-04 - acc: 1.0000 - val_loss: 0.0483 - val_acc: 0.9931\n",
            "Epoch 53/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.8356e-04 - acc: 1.0000 - val_loss: 0.0484 - val_acc: 0.9932\n",
            "Epoch 54/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.8303e-04 - acc: 1.0000 - val_loss: 0.0484 - val_acc: 0.9932\n",
            "Epoch 55/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.8253e-04 - acc: 1.0000 - val_loss: 0.0485 - val_acc: 0.9932\n",
            "Epoch 56/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.8204e-04 - acc: 1.0000 - val_loss: 0.0485 - val_acc: 0.9933\n",
            "Epoch 57/500\n",
            "60000/60000 [==============================] - 3s 44us/step - loss: 2.8154e-04 - acc: 1.0000 - val_loss: 0.0486 - val_acc: 0.9933\n",
            "Epoch 58/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.8108e-04 - acc: 1.0000 - val_loss: 0.0487 - val_acc: 0.9933\n",
            "Epoch 59/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.8064e-04 - acc: 1.0000 - val_loss: 0.0487 - val_acc: 0.9933\n",
            "Epoch 60/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.8023e-04 - acc: 1.0000 - val_loss: 0.0488 - val_acc: 0.9934\n",
            "Epoch 61/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.7982e-04 - acc: 1.0000 - val_loss: 0.0488 - val_acc: 0.9934\n",
            "Epoch 62/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.7943e-04 - acc: 1.0000 - val_loss: 0.0489 - val_acc: 0.9934\n",
            "Epoch 63/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.7906e-04 - acc: 1.0000 - val_loss: 0.0489 - val_acc: 0.9934\n",
            "Epoch 64/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.7869e-04 - acc: 1.0000 - val_loss: 0.0490 - val_acc: 0.9933\n",
            "Epoch 65/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.7835e-04 - acc: 1.0000 - val_loss: 0.0490 - val_acc: 0.9933\n",
            "Epoch 66/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.7802e-04 - acc: 1.0000 - val_loss: 0.0491 - val_acc: 0.9933\n",
            "Epoch 67/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.7769e-04 - acc: 1.0000 - val_loss: 0.0492 - val_acc: 0.9933\n",
            "Epoch 68/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.7739e-04 - acc: 1.0000 - val_loss: 0.0492 - val_acc: 0.9933\n",
            "Epoch 69/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.7709e-04 - acc: 1.0000 - val_loss: 0.0493 - val_acc: 0.9933\n",
            "Epoch 70/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.7680e-04 - acc: 1.0000 - val_loss: 0.0494 - val_acc: 0.9932\n",
            "Epoch 71/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.7652e-04 - acc: 1.0000 - val_loss: 0.0494 - val_acc: 0.9932\n",
            "Epoch 72/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.7625e-04 - acc: 1.0000 - val_loss: 0.0495 - val_acc: 0.9932\n",
            "Epoch 73/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.7600e-04 - acc: 1.0000 - val_loss: 0.0496 - val_acc: 0.9933\n",
            "Epoch 74/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.7575e-04 - acc: 1.0000 - val_loss: 0.0496 - val_acc: 0.9932\n",
            "Epoch 75/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.7552e-04 - acc: 1.0000 - val_loss: 0.0497 - val_acc: 0.9932\n",
            "Epoch 76/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.7527e-04 - acc: 1.0000 - val_loss: 0.0497 - val_acc: 0.9932\n",
            "Epoch 77/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.7505e-04 - acc: 1.0000 - val_loss: 0.0498 - val_acc: 0.9932\n",
            "Epoch 78/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.7483e-04 - acc: 1.0000 - val_loss: 0.0498 - val_acc: 0.9933\n",
            "Epoch 79/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.7463e-04 - acc: 1.0000 - val_loss: 0.0499 - val_acc: 0.9933\n",
            "Epoch 80/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.7442e-04 - acc: 1.0000 - val_loss: 0.0500 - val_acc: 0.9933\n",
            "Epoch 81/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.7424e-04 - acc: 1.0000 - val_loss: 0.0500 - val_acc: 0.9933\n",
            "Epoch 82/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.7405e-04 - acc: 1.0000 - val_loss: 0.0501 - val_acc: 0.9933\n",
            "Epoch 83/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.7387e-04 - acc: 1.0000 - val_loss: 0.0501 - val_acc: 0.9933\n",
            "Epoch 84/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.7369e-04 - acc: 1.0000 - val_loss: 0.0502 - val_acc: 0.9933\n",
            "Epoch 85/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.7352e-04 - acc: 1.0000 - val_loss: 0.0503 - val_acc: 0.9933\n",
            "Epoch 86/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.7336e-04 - acc: 1.0000 - val_loss: 0.0503 - val_acc: 0.9933\n",
            "Epoch 87/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.7320e-04 - acc: 1.0000 - val_loss: 0.0504 - val_acc: 0.9933\n",
            "Epoch 88/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.7305e-04 - acc: 1.0000 - val_loss: 0.0504 - val_acc: 0.9933\n",
            "Epoch 89/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.7290e-04 - acc: 1.0000 - val_loss: 0.0505 - val_acc: 0.9933\n",
            "Epoch 90/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.7276e-04 - acc: 1.0000 - val_loss: 0.0505 - val_acc: 0.9933\n",
            "Epoch 91/500\n",
            "60000/60000 [==============================] - 3s 44us/step - loss: 2.7263e-04 - acc: 1.0000 - val_loss: 0.0506 - val_acc: 0.9933\n",
            "Epoch 92/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.7250e-04 - acc: 1.0000 - val_loss: 0.0507 - val_acc: 0.9933\n",
            "Epoch 93/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.7237e-04 - acc: 1.0000 - val_loss: 0.0507 - val_acc: 0.9933\n",
            "Epoch 94/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.7225e-04 - acc: 1.0000 - val_loss: 0.0508 - val_acc: 0.9933\n",
            "Epoch 95/500\n",
            "60000/60000 [==============================] - 3s 44us/step - loss: 2.7214e-04 - acc: 1.0000 - val_loss: 0.0509 - val_acc: 0.9933\n",
            "Epoch 96/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.7202e-04 - acc: 1.0000 - val_loss: 0.0509 - val_acc: 0.9933\n",
            "Epoch 97/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.7191e-04 - acc: 1.0000 - val_loss: 0.0510 - val_acc: 0.9933\n",
            "Epoch 98/500\n",
            "60000/60000 [==============================] - 3s 44us/step - loss: 2.7180e-04 - acc: 1.0000 - val_loss: 0.0510 - val_acc: 0.9933\n",
            "Epoch 99/500\n",
            "60000/60000 [==============================] - 3s 44us/step - loss: 2.7170e-04 - acc: 1.0000 - val_loss: 0.0511 - val_acc: 0.9933\n",
            "Epoch 100/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.7161e-04 - acc: 1.0000 - val_loss: 0.0511 - val_acc: 0.9933\n",
            "Epoch 101/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.7151e-04 - acc: 1.0000 - val_loss: 0.0512 - val_acc: 0.9933\n",
            "Epoch 102/500\n",
            "60000/60000 [==============================] - 3s 44us/step - loss: 2.7142e-04 - acc: 1.0000 - val_loss: 0.0513 - val_acc: 0.9933\n",
            "Epoch 103/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.7133e-04 - acc: 1.0000 - val_loss: 0.0513 - val_acc: 0.9933\n",
            "Epoch 104/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.7124e-04 - acc: 1.0000 - val_loss: 0.0513 - val_acc: 0.9933\n",
            "Epoch 105/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.7116e-04 - acc: 1.0000 - val_loss: 0.0514 - val_acc: 0.9933\n",
            "Epoch 106/500\n",
            "60000/60000 [==============================] - 3s 44us/step - loss: 2.7108e-04 - acc: 1.0000 - val_loss: 0.0515 - val_acc: 0.9933\n",
            "Epoch 107/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.7100e-04 - acc: 1.0000 - val_loss: 0.0515 - val_acc: 0.9933\n",
            "Epoch 108/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.7092e-04 - acc: 1.0000 - val_loss: 0.0516 - val_acc: 0.9933\n",
            "Epoch 109/500\n",
            "60000/60000 [==============================] - 3s 44us/step - loss: 2.7085e-04 - acc: 1.0000 - val_loss: 0.0516 - val_acc: 0.9933\n",
            "Epoch 110/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.7078e-04 - acc: 1.0000 - val_loss: 0.0517 - val_acc: 0.9933\n",
            "Epoch 111/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.7071e-04 - acc: 1.0000 - val_loss: 0.0518 - val_acc: 0.9933\n",
            "Epoch 112/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.7065e-04 - acc: 1.0000 - val_loss: 0.0518 - val_acc: 0.9933\n",
            "Epoch 113/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.7058e-04 - acc: 1.0000 - val_loss: 0.0519 - val_acc: 0.9933\n",
            "Epoch 114/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.7052e-04 - acc: 1.0000 - val_loss: 0.0519 - val_acc: 0.9933\n",
            "Epoch 115/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.7046e-04 - acc: 1.0000 - val_loss: 0.0520 - val_acc: 0.9933\n",
            "Epoch 116/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.7040e-04 - acc: 1.0000 - val_loss: 0.0520 - val_acc: 0.9933\n",
            "Epoch 117/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.7035e-04 - acc: 1.0000 - val_loss: 0.0521 - val_acc: 0.9932\n",
            "Epoch 118/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.7030e-04 - acc: 1.0000 - val_loss: 0.0522 - val_acc: 0.9932\n",
            "Epoch 119/500\n",
            "60000/60000 [==============================] - 3s 44us/step - loss: 2.7025e-04 - acc: 1.0000 - val_loss: 0.0522 - val_acc: 0.9932\n",
            "Epoch 120/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.7020e-04 - acc: 1.0000 - val_loss: 0.0523 - val_acc: 0.9933\n",
            "Epoch 121/500\n",
            "60000/60000 [==============================] - 3s 44us/step - loss: 2.7015e-04 - acc: 1.0000 - val_loss: 0.0523 - val_acc: 0.9933\n",
            "Epoch 122/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.7010e-04 - acc: 1.0000 - val_loss: 0.0524 - val_acc: 0.9933\n",
            "Epoch 123/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.7006e-04 - acc: 1.0000 - val_loss: 0.0524 - val_acc: 0.9933\n",
            "Epoch 124/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.7002e-04 - acc: 1.0000 - val_loss: 0.0525 - val_acc: 0.9933\n",
            "Epoch 125/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6997e-04 - acc: 1.0000 - val_loss: 0.0525 - val_acc: 0.9933\n",
            "Epoch 126/500\n",
            "60000/60000 [==============================] - 3s 44us/step - loss: 2.6993e-04 - acc: 1.0000 - val_loss: 0.0526 - val_acc: 0.9933\n",
            "Epoch 127/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6989e-04 - acc: 1.0000 - val_loss: 0.0527 - val_acc: 0.9933\n",
            "Epoch 128/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6986e-04 - acc: 1.0000 - val_loss: 0.0527 - val_acc: 0.9933\n",
            "Epoch 129/500\n",
            "60000/60000 [==============================] - 3s 44us/step - loss: 2.6982e-04 - acc: 1.0000 - val_loss: 0.0528 - val_acc: 0.9933\n",
            "Epoch 130/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6978e-04 - acc: 1.0000 - val_loss: 0.0528 - val_acc: 0.9933\n",
            "Epoch 131/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6975e-04 - acc: 1.0000 - val_loss: 0.0529 - val_acc: 0.9933\n",
            "Epoch 132/500\n",
            "60000/60000 [==============================] - 3s 44us/step - loss: 2.6972e-04 - acc: 1.0000 - val_loss: 0.0529 - val_acc: 0.9933\n",
            "Epoch 133/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6968e-04 - acc: 1.0000 - val_loss: 0.0530 - val_acc: 0.9933\n",
            "Epoch 134/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6965e-04 - acc: 1.0000 - val_loss: 0.0531 - val_acc: 0.9933\n",
            "Epoch 135/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6962e-04 - acc: 1.0000 - val_loss: 0.0531 - val_acc: 0.9933\n",
            "Epoch 136/500\n",
            "60000/60000 [==============================] - 3s 44us/step - loss: 2.6959e-04 - acc: 1.0000 - val_loss: 0.0532 - val_acc: 0.9933\n",
            "Epoch 137/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6956e-04 - acc: 1.0000 - val_loss: 0.0532 - val_acc: 0.9933\n",
            "Epoch 138/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6954e-04 - acc: 1.0000 - val_loss: 0.0533 - val_acc: 0.9933\n",
            "Epoch 139/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6951e-04 - acc: 1.0000 - val_loss: 0.0533 - val_acc: 0.9933\n",
            "Epoch 140/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6949e-04 - acc: 1.0000 - val_loss: 0.0534 - val_acc: 0.9933\n",
            "Epoch 141/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6946e-04 - acc: 1.0000 - val_loss: 0.0534 - val_acc: 0.9933\n",
            "Epoch 142/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6944e-04 - acc: 1.0000 - val_loss: 0.0535 - val_acc: 0.9932\n",
            "Epoch 143/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6941e-04 - acc: 1.0000 - val_loss: 0.0536 - val_acc: 0.9932\n",
            "Epoch 144/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6939e-04 - acc: 1.0000 - val_loss: 0.0536 - val_acc: 0.9932\n",
            "Epoch 145/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6937e-04 - acc: 1.0000 - val_loss: 0.0537 - val_acc: 0.9932\n",
            "Epoch 146/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6935e-04 - acc: 1.0000 - val_loss: 0.0537 - val_acc: 0.9932\n",
            "Epoch 147/500\n",
            "60000/60000 [==============================] - 3s 44us/step - loss: 2.6933e-04 - acc: 1.0000 - val_loss: 0.0538 - val_acc: 0.9932\n",
            "Epoch 148/500\n",
            "60000/60000 [==============================] - 3s 44us/step - loss: 2.6931e-04 - acc: 1.0000 - val_loss: 0.0538 - val_acc: 0.9932\n",
            "Epoch 149/500\n",
            "60000/60000 [==============================] - 3s 44us/step - loss: 2.6929e-04 - acc: 1.0000 - val_loss: 0.0539 - val_acc: 0.9932\n",
            "Epoch 150/500\n",
            "60000/60000 [==============================] - 3s 44us/step - loss: 2.6927e-04 - acc: 1.0000 - val_loss: 0.0539 - val_acc: 0.9932\n",
            "Epoch 151/500\n",
            "60000/60000 [==============================] - 3s 44us/step - loss: 2.6926e-04 - acc: 1.0000 - val_loss: 0.0540 - val_acc: 0.9932\n",
            "Epoch 152/500\n",
            "60000/60000 [==============================] - 3s 44us/step - loss: 2.6924e-04 - acc: 1.0000 - val_loss: 0.0540 - val_acc: 0.9932\n",
            "Epoch 153/500\n",
            "60000/60000 [==============================] - 3s 44us/step - loss: 2.6922e-04 - acc: 1.0000 - val_loss: 0.0541 - val_acc: 0.9932\n",
            "Epoch 154/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6921e-04 - acc: 1.0000 - val_loss: 0.0541 - val_acc: 0.9932\n",
            "Epoch 155/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6919e-04 - acc: 1.0000 - val_loss: 0.0542 - val_acc: 0.9932\n",
            "Epoch 156/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6918e-04 - acc: 1.0000 - val_loss: 0.0543 - val_acc: 0.9932\n",
            "Epoch 157/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6917e-04 - acc: 1.0000 - val_loss: 0.0543 - val_acc: 0.9932\n",
            "Epoch 158/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6915e-04 - acc: 1.0000 - val_loss: 0.0544 - val_acc: 0.9932\n",
            "Epoch 159/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6914e-04 - acc: 1.0000 - val_loss: 0.0544 - val_acc: 0.9932\n",
            "Epoch 160/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6913e-04 - acc: 1.0000 - val_loss: 0.0545 - val_acc: 0.9932\n",
            "Epoch 161/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6911e-04 - acc: 1.0000 - val_loss: 0.0545 - val_acc: 0.9932\n",
            "Epoch 162/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6910e-04 - acc: 1.0000 - val_loss: 0.0546 - val_acc: 0.9932\n",
            "Epoch 163/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6909e-04 - acc: 1.0000 - val_loss: 0.0546 - val_acc: 0.9932\n",
            "Epoch 164/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6908e-04 - acc: 1.0000 - val_loss: 0.0547 - val_acc: 0.9932\n",
            "Epoch 165/500\n",
            "60000/60000 [==============================] - 3s 44us/step - loss: 2.6907e-04 - acc: 1.0000 - val_loss: 0.0547 - val_acc: 0.9932\n",
            "Epoch 166/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6906e-04 - acc: 1.0000 - val_loss: 0.0548 - val_acc: 0.9932\n",
            "Epoch 167/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6905e-04 - acc: 1.0000 - val_loss: 0.0548 - val_acc: 0.9933\n",
            "Epoch 168/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6904e-04 - acc: 1.0000 - val_loss: 0.0549 - val_acc: 0.9932\n",
            "Epoch 169/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6903e-04 - acc: 1.0000 - val_loss: 0.0549 - val_acc: 0.9932\n",
            "Epoch 170/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6902e-04 - acc: 1.0000 - val_loss: 0.0550 - val_acc: 0.9933\n",
            "Epoch 171/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6901e-04 - acc: 1.0000 - val_loss: 0.0550 - val_acc: 0.9933\n",
            "Epoch 172/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6900e-04 - acc: 1.0000 - val_loss: 0.0551 - val_acc: 0.9933\n",
            "Epoch 173/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6900e-04 - acc: 1.0000 - val_loss: 0.0551 - val_acc: 0.9933\n",
            "Epoch 174/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6899e-04 - acc: 1.0000 - val_loss: 0.0551 - val_acc: 0.9933\n",
            "Epoch 175/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6898e-04 - acc: 1.0000 - val_loss: 0.0552 - val_acc: 0.9933\n",
            "Epoch 176/500\n",
            "60000/60000 [==============================] - 3s 44us/step - loss: 2.6897e-04 - acc: 1.0000 - val_loss: 0.0552 - val_acc: 0.9933\n",
            "Epoch 177/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6897e-04 - acc: 1.0000 - val_loss: 0.0553 - val_acc: 0.9933\n",
            "Epoch 178/500\n",
            "60000/60000 [==============================] - 3s 44us/step - loss: 2.6896e-04 - acc: 1.0000 - val_loss: 0.0553 - val_acc: 0.9933\n",
            "Epoch 179/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6895e-04 - acc: 1.0000 - val_loss: 0.0554 - val_acc: 0.9933\n",
            "Epoch 180/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6895e-04 - acc: 1.0000 - val_loss: 0.0554 - val_acc: 0.9933\n",
            "Epoch 181/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6894e-04 - acc: 1.0000 - val_loss: 0.0555 - val_acc: 0.9933\n",
            "Epoch 182/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6893e-04 - acc: 1.0000 - val_loss: 0.0555 - val_acc: 0.9933\n",
            "Epoch 183/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6893e-04 - acc: 1.0000 - val_loss: 0.0556 - val_acc: 0.9933\n",
            "Epoch 184/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6892e-04 - acc: 1.0000 - val_loss: 0.0556 - val_acc: 0.9933\n",
            "Epoch 185/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6892e-04 - acc: 1.0000 - val_loss: 0.0557 - val_acc: 0.9933\n",
            "Epoch 186/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6891e-04 - acc: 1.0000 - val_loss: 0.0557 - val_acc: 0.9933\n",
            "Epoch 187/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6891e-04 - acc: 1.0000 - val_loss: 0.0557 - val_acc: 0.9933\n",
            "Epoch 188/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6890e-04 - acc: 1.0000 - val_loss: 0.0558 - val_acc: 0.9933\n",
            "Epoch 189/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6890e-04 - acc: 1.0000 - val_loss: 0.0558 - val_acc: 0.9933\n",
            "Epoch 190/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6889e-04 - acc: 1.0000 - val_loss: 0.0559 - val_acc: 0.9933\n",
            "Epoch 191/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6889e-04 - acc: 1.0000 - val_loss: 0.0559 - val_acc: 0.9933\n",
            "Epoch 192/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6888e-04 - acc: 1.0000 - val_loss: 0.0560 - val_acc: 0.9933\n",
            "Epoch 193/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6888e-04 - acc: 1.0000 - val_loss: 0.0560 - val_acc: 0.9933\n",
            "Epoch 194/500\n",
            "60000/60000 [==============================] - 3s 44us/step - loss: 2.6888e-04 - acc: 1.0000 - val_loss: 0.0561 - val_acc: 0.9933\n",
            "Epoch 195/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6887e-04 - acc: 1.0000 - val_loss: 0.0561 - val_acc: 0.9933\n",
            "Epoch 196/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6887e-04 - acc: 1.0000 - val_loss: 0.0562 - val_acc: 0.9933\n",
            "Epoch 197/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6886e-04 - acc: 1.0000 - val_loss: 0.0562 - val_acc: 0.9933\n",
            "Epoch 198/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6886e-04 - acc: 1.0000 - val_loss: 0.0563 - val_acc: 0.9933\n",
            "Epoch 199/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6886e-04 - acc: 1.0000 - val_loss: 0.0563 - val_acc: 0.9933\n",
            "Epoch 200/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6885e-04 - acc: 1.0000 - val_loss: 0.0564 - val_acc: 0.9933\n",
            "Epoch 201/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6885e-04 - acc: 1.0000 - val_loss: 0.0564 - val_acc: 0.9933\n",
            "Epoch 202/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6885e-04 - acc: 1.0000 - val_loss: 0.0565 - val_acc: 0.9934\n",
            "Epoch 203/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6884e-04 - acc: 1.0000 - val_loss: 0.0565 - val_acc: 0.9934\n",
            "Epoch 204/500\n",
            "60000/60000 [==============================] - 3s 44us/step - loss: 2.6884e-04 - acc: 1.0000 - val_loss: 0.0566 - val_acc: 0.9934\n",
            "Epoch 205/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6884e-04 - acc: 1.0000 - val_loss: 0.0566 - val_acc: 0.9934\n",
            "Epoch 206/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6884e-04 - acc: 1.0000 - val_loss: 0.0567 - val_acc: 0.9934\n",
            "Epoch 207/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6883e-04 - acc: 1.0000 - val_loss: 0.0567 - val_acc: 0.9934\n",
            "Epoch 208/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6883e-04 - acc: 1.0000 - val_loss: 0.0567 - val_acc: 0.9934\n",
            "Epoch 209/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6883e-04 - acc: 1.0000 - val_loss: 0.0568 - val_acc: 0.9934\n",
            "Epoch 210/500\n",
            "60000/60000 [==============================] - 3s 44us/step - loss: 2.6883e-04 - acc: 1.0000 - val_loss: 0.0569 - val_acc: 0.9934\n",
            "Epoch 211/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6882e-04 - acc: 1.0000 - val_loss: 0.0569 - val_acc: 0.9934\n",
            "Epoch 212/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6882e-04 - acc: 1.0000 - val_loss: 0.0569 - val_acc: 0.9934\n",
            "Epoch 213/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6882e-04 - acc: 1.0000 - val_loss: 0.0569 - val_acc: 0.9934\n",
            "Epoch 214/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6882e-04 - acc: 1.0000 - val_loss: 0.0570 - val_acc: 0.9934\n",
            "Epoch 215/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6881e-04 - acc: 1.0000 - val_loss: 0.0570 - val_acc: 0.9934\n",
            "Epoch 216/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6881e-04 - acc: 1.0000 - val_loss: 0.0570 - val_acc: 0.9934\n",
            "Epoch 217/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6881e-04 - acc: 1.0000 - val_loss: 0.0571 - val_acc: 0.9934\n",
            "Epoch 218/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6881e-04 - acc: 1.0000 - val_loss: 0.0571 - val_acc: 0.9934\n",
            "Epoch 219/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6881e-04 - acc: 1.0000 - val_loss: 0.0572 - val_acc: 0.9934\n",
            "Epoch 220/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6881e-04 - acc: 1.0000 - val_loss: 0.0572 - val_acc: 0.9934\n",
            "Epoch 221/500\n",
            "60000/60000 [==============================] - 3s 44us/step - loss: 2.6880e-04 - acc: 1.0000 - val_loss: 0.0573 - val_acc: 0.9934\n",
            "Epoch 222/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6880e-04 - acc: 1.0000 - val_loss: 0.0573 - val_acc: 0.9934\n",
            "Epoch 223/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6880e-04 - acc: 1.0000 - val_loss: 0.0573 - val_acc: 0.9934\n",
            "Epoch 224/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6880e-04 - acc: 1.0000 - val_loss: 0.0574 - val_acc: 0.9934\n",
            "Epoch 225/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6880e-04 - acc: 1.0000 - val_loss: 0.0574 - val_acc: 0.9934\n",
            "Epoch 226/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6880e-04 - acc: 1.0000 - val_loss: 0.0575 - val_acc: 0.9934\n",
            "Epoch 227/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6879e-04 - acc: 1.0000 - val_loss: 0.0575 - val_acc: 0.9934\n",
            "Epoch 228/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6879e-04 - acc: 1.0000 - val_loss: 0.0576 - val_acc: 0.9934\n",
            "Epoch 229/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6879e-04 - acc: 1.0000 - val_loss: 0.0576 - val_acc: 0.9934\n",
            "Epoch 230/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6879e-04 - acc: 1.0000 - val_loss: 0.0576 - val_acc: 0.9934\n",
            "Epoch 231/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6879e-04 - acc: 1.0000 - val_loss: 0.0577 - val_acc: 0.9934\n",
            "Epoch 232/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6879e-04 - acc: 1.0000 - val_loss: 0.0578 - val_acc: 0.9934\n",
            "Epoch 233/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6879e-04 - acc: 1.0000 - val_loss: 0.0578 - val_acc: 0.9934\n",
            "Epoch 234/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6879e-04 - acc: 1.0000 - val_loss: 0.0578 - val_acc: 0.9933\n",
            "Epoch 235/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6878e-04 - acc: 1.0000 - val_loss: 0.0579 - val_acc: 0.9932\n",
            "Epoch 236/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6878e-04 - acc: 1.0000 - val_loss: 0.0579 - val_acc: 0.9931\n",
            "Epoch 237/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6878e-04 - acc: 1.0000 - val_loss: 0.0580 - val_acc: 0.9931\n",
            "Epoch 238/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6878e-04 - acc: 1.0000 - val_loss: 0.0580 - val_acc: 0.9931\n",
            "Epoch 239/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6878e-04 - acc: 1.0000 - val_loss: 0.0581 - val_acc: 0.9931\n",
            "Epoch 240/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6878e-04 - acc: 1.0000 - val_loss: 0.0581 - val_acc: 0.9931\n",
            "Epoch 241/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6878e-04 - acc: 1.0000 - val_loss: 0.0581 - val_acc: 0.9931\n",
            "Epoch 242/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6878e-04 - acc: 1.0000 - val_loss: 0.0582 - val_acc: 0.9931\n",
            "Epoch 243/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6878e-04 - acc: 1.0000 - val_loss: 0.0582 - val_acc: 0.9931\n",
            "Epoch 244/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6878e-04 - acc: 1.0000 - val_loss: 0.0583 - val_acc: 0.9931\n",
            "Epoch 245/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6878e-04 - acc: 1.0000 - val_loss: 0.0583 - val_acc: 0.9931\n",
            "Epoch 246/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6878e-04 - acc: 1.0000 - val_loss: 0.0584 - val_acc: 0.9931\n",
            "Epoch 247/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6877e-04 - acc: 1.0000 - val_loss: 0.0584 - val_acc: 0.9931\n",
            "Epoch 248/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6877e-04 - acc: 1.0000 - val_loss: 0.0584 - val_acc: 0.9931\n",
            "Epoch 249/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6877e-04 - acc: 1.0000 - val_loss: 0.0585 - val_acc: 0.9931\n",
            "Epoch 250/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6877e-04 - acc: 1.0000 - val_loss: 0.0585 - val_acc: 0.9931\n",
            "Epoch 251/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6877e-04 - acc: 1.0000 - val_loss: 0.0586 - val_acc: 0.9931\n",
            "Epoch 252/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6877e-04 - acc: 1.0000 - val_loss: 0.0586 - val_acc: 0.9931\n",
            "Epoch 253/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6877e-04 - acc: 1.0000 - val_loss: 0.0587 - val_acc: 0.9931\n",
            "Epoch 254/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6877e-04 - acc: 1.0000 - val_loss: 0.0587 - val_acc: 0.9931\n",
            "Epoch 255/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6877e-04 - acc: 1.0000 - val_loss: 0.0587 - val_acc: 0.9931\n",
            "Epoch 256/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6877e-04 - acc: 1.0000 - val_loss: 0.0588 - val_acc: 0.9931\n",
            "Epoch 257/500\n",
            "60000/60000 [==============================] - 3s 44us/step - loss: 2.6877e-04 - acc: 1.0000 - val_loss: 0.0588 - val_acc: 0.9931\n",
            "Epoch 258/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6877e-04 - acc: 1.0000 - val_loss: 0.0589 - val_acc: 0.9931\n",
            "Epoch 259/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6877e-04 - acc: 1.0000 - val_loss: 0.0589 - val_acc: 0.9931\n",
            "Epoch 260/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6877e-04 - acc: 1.0000 - val_loss: 0.0590 - val_acc: 0.9931\n",
            "Epoch 261/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6877e-04 - acc: 1.0000 - val_loss: 0.0590 - val_acc: 0.9931\n",
            "Epoch 262/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6877e-04 - acc: 1.0000 - val_loss: 0.0591 - val_acc: 0.9931\n",
            "Epoch 263/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6877e-04 - acc: 1.0000 - val_loss: 0.0591 - val_acc: 0.9931\n",
            "Epoch 264/500\n",
            "60000/60000 [==============================] - 3s 44us/step - loss: 2.6877e-04 - acc: 1.0000 - val_loss: 0.0592 - val_acc: 0.9931\n",
            "Epoch 265/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6877e-04 - acc: 1.0000 - val_loss: 0.0592 - val_acc: 0.9931\n",
            "Epoch 266/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6876e-04 - acc: 1.0000 - val_loss: 0.0592 - val_acc: 0.9931\n",
            "Epoch 267/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6876e-04 - acc: 1.0000 - val_loss: 0.0593 - val_acc: 0.9931\n",
            "Epoch 268/500\n",
            "60000/60000 [==============================] - 3s 44us/step - loss: 2.6876e-04 - acc: 1.0000 - val_loss: 0.0593 - val_acc: 0.9931\n",
            "Epoch 269/500\n",
            "60000/60000 [==============================] - 3s 44us/step - loss: 2.6876e-04 - acc: 1.0000 - val_loss: 0.0593 - val_acc: 0.9931\n",
            "Epoch 270/500\n",
            "60000/60000 [==============================] - 3s 44us/step - loss: 2.6876e-04 - acc: 1.0000 - val_loss: 0.0594 - val_acc: 0.9931\n",
            "Epoch 271/500\n",
            "60000/60000 [==============================] - 3s 44us/step - loss: 2.6876e-04 - acc: 1.0000 - val_loss: 0.0595 - val_acc: 0.9931\n",
            "Epoch 272/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6876e-04 - acc: 1.0000 - val_loss: 0.0595 - val_acc: 0.9931\n",
            "Epoch 273/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6876e-04 - acc: 1.0000 - val_loss: 0.0595 - val_acc: 0.9931\n",
            "Epoch 274/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6876e-04 - acc: 1.0000 - val_loss: 0.0596 - val_acc: 0.9931\n",
            "Epoch 275/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6876e-04 - acc: 1.0000 - val_loss: 0.0596 - val_acc: 0.9931\n",
            "Epoch 276/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6876e-04 - acc: 1.0000 - val_loss: 0.0597 - val_acc: 0.9931\n",
            "Epoch 277/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6876e-04 - acc: 1.0000 - val_loss: 0.0597 - val_acc: 0.9931\n",
            "Epoch 278/500\n",
            "60000/60000 [==============================] - 3s 44us/step - loss: 2.6876e-04 - acc: 1.0000 - val_loss: 0.0597 - val_acc: 0.9931\n",
            "Epoch 279/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6876e-04 - acc: 1.0000 - val_loss: 0.0598 - val_acc: 0.9931\n",
            "Epoch 280/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6876e-04 - acc: 1.0000 - val_loss: 0.0598 - val_acc: 0.9931\n",
            "Epoch 281/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6876e-04 - acc: 1.0000 - val_loss: 0.0599 - val_acc: 0.9931\n",
            "Epoch 282/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6876e-04 - acc: 1.0000 - val_loss: 0.0599 - val_acc: 0.9931\n",
            "Epoch 283/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6876e-04 - acc: 1.0000 - val_loss: 0.0600 - val_acc: 0.9931\n",
            "Epoch 284/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6876e-04 - acc: 1.0000 - val_loss: 0.0600 - val_acc: 0.9931\n",
            "Epoch 285/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6876e-04 - acc: 1.0000 - val_loss: 0.0600 - val_acc: 0.9931\n",
            "Epoch 286/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6876e-04 - acc: 1.0000 - val_loss: 0.0601 - val_acc: 0.9931\n",
            "Epoch 287/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6876e-04 - acc: 1.0000 - val_loss: 0.0601 - val_acc: 0.9931\n",
            "Epoch 288/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6876e-04 - acc: 1.0000 - val_loss: 0.0602 - val_acc: 0.9931\n",
            "Epoch 289/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6876e-04 - acc: 1.0000 - val_loss: 0.0602 - val_acc: 0.9931\n",
            "Epoch 290/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6876e-04 - acc: 1.0000 - val_loss: 0.0602 - val_acc: 0.9931\n",
            "Epoch 291/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6876e-04 - acc: 1.0000 - val_loss: 0.0603 - val_acc: 0.9931\n",
            "Epoch 292/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6876e-04 - acc: 1.0000 - val_loss: 0.0603 - val_acc: 0.9931\n",
            "Epoch 293/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6876e-04 - acc: 1.0000 - val_loss: 0.0604 - val_acc: 0.9931\n",
            "Epoch 294/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6876e-04 - acc: 1.0000 - val_loss: 0.0604 - val_acc: 0.9931\n",
            "Epoch 295/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6876e-04 - acc: 1.0000 - val_loss: 0.0605 - val_acc: 0.9931\n",
            "Epoch 296/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6876e-04 - acc: 1.0000 - val_loss: 0.0605 - val_acc: 0.9931\n",
            "Epoch 297/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6876e-04 - acc: 1.0000 - val_loss: 0.0605 - val_acc: 0.9931\n",
            "Epoch 298/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6876e-04 - acc: 1.0000 - val_loss: 0.0605 - val_acc: 0.9931\n",
            "Epoch 299/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6876e-04 - acc: 1.0000 - val_loss: 0.0606 - val_acc: 0.9931\n",
            "Epoch 300/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6876e-04 - acc: 1.0000 - val_loss: 0.0606 - val_acc: 0.9931\n",
            "Epoch 301/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6876e-04 - acc: 1.0000 - val_loss: 0.0607 - val_acc: 0.9931\n",
            "Epoch 302/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6876e-04 - acc: 1.0000 - val_loss: 0.0607 - val_acc: 0.9931\n",
            "Epoch 303/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6876e-04 - acc: 1.0000 - val_loss: 0.0607 - val_acc: 0.9932\n",
            "Epoch 304/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6876e-04 - acc: 1.0000 - val_loss: 0.0608 - val_acc: 0.9932\n",
            "Epoch 305/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6876e-04 - acc: 1.0000 - val_loss: 0.0608 - val_acc: 0.9932\n",
            "Epoch 306/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6876e-04 - acc: 1.0000 - val_loss: 0.0608 - val_acc: 0.9932\n",
            "Epoch 307/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6876e-04 - acc: 1.0000 - val_loss: 0.0609 - val_acc: 0.9932\n",
            "Epoch 308/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6876e-04 - acc: 1.0000 - val_loss: 0.0609 - val_acc: 0.9932\n",
            "Epoch 309/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6876e-04 - acc: 1.0000 - val_loss: 0.0609 - val_acc: 0.9932\n",
            "Epoch 310/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6876e-04 - acc: 1.0000 - val_loss: 0.0610 - val_acc: 0.9932\n",
            "Epoch 311/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6876e-04 - acc: 1.0000 - val_loss: 0.0610 - val_acc: 0.9932\n",
            "Epoch 312/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6876e-04 - acc: 1.0000 - val_loss: 0.0610 - val_acc: 0.9932\n",
            "Epoch 313/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6876e-04 - acc: 1.0000 - val_loss: 0.0611 - val_acc: 0.9932\n",
            "Epoch 314/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6876e-04 - acc: 1.0000 - val_loss: 0.0611 - val_acc: 0.9932\n",
            "Epoch 315/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6876e-04 - acc: 1.0000 - val_loss: 0.0611 - val_acc: 0.9932\n",
            "Epoch 316/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6876e-04 - acc: 1.0000 - val_loss: 0.0612 - val_acc: 0.9932\n",
            "Epoch 317/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6876e-04 - acc: 1.0000 - val_loss: 0.0612 - val_acc: 0.9932\n",
            "Epoch 318/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6876e-04 - acc: 1.0000 - val_loss: 0.0612 - val_acc: 0.9932\n",
            "Epoch 319/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6876e-04 - acc: 1.0000 - val_loss: 0.0613 - val_acc: 0.9932\n",
            "Epoch 320/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6876e-04 - acc: 1.0000 - val_loss: 0.0613 - val_acc: 0.9932\n",
            "Epoch 321/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6876e-04 - acc: 1.0000 - val_loss: 0.0613 - val_acc: 0.9932\n",
            "Epoch 322/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6876e-04 - acc: 1.0000 - val_loss: 0.0613 - val_acc: 0.9932\n",
            "Epoch 323/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6876e-04 - acc: 1.0000 - val_loss: 0.0614 - val_acc: 0.9932\n",
            "Epoch 324/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6876e-04 - acc: 1.0000 - val_loss: 0.0614 - val_acc: 0.9932\n",
            "Epoch 325/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6876e-04 - acc: 1.0000 - val_loss: 0.0614 - val_acc: 0.9932\n",
            "Epoch 326/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6876e-04 - acc: 1.0000 - val_loss: 0.0615 - val_acc: 0.9932\n",
            "Epoch 327/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6876e-04 - acc: 1.0000 - val_loss: 0.0615 - val_acc: 0.9932\n",
            "Epoch 328/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6876e-04 - acc: 1.0000 - val_loss: 0.0615 - val_acc: 0.9932\n",
            "Epoch 329/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0615 - val_acc: 0.9932\n",
            "Epoch 330/500\n",
            "60000/60000 [==============================] - 3s 44us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0616 - val_acc: 0.9932\n",
            "Epoch 331/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0616 - val_acc: 0.9932\n",
            "Epoch 332/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0616 - val_acc: 0.9931\n",
            "Epoch 333/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0617 - val_acc: 0.9931\n",
            "Epoch 334/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0617 - val_acc: 0.9931\n",
            "Epoch 335/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0617 - val_acc: 0.9931\n",
            "Epoch 336/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0617 - val_acc: 0.9931\n",
            "Epoch 337/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0618 - val_acc: 0.9931\n",
            "Epoch 338/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0618 - val_acc: 0.9931\n",
            "Epoch 339/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0618 - val_acc: 0.9931\n",
            "Epoch 340/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0618 - val_acc: 0.9931\n",
            "Epoch 341/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0619 - val_acc: 0.9931\n",
            "Epoch 342/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0619 - val_acc: 0.9931\n",
            "Epoch 343/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0619 - val_acc: 0.9932\n",
            "Epoch 344/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0620 - val_acc: 0.9932\n",
            "Epoch 345/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0620 - val_acc: 0.9932\n",
            "Epoch 346/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0620 - val_acc: 0.9932\n",
            "Epoch 347/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0620 - val_acc: 0.9932\n",
            "Epoch 348/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0621 - val_acc: 0.9932\n",
            "Epoch 349/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0621 - val_acc: 0.9932\n",
            "Epoch 350/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0621 - val_acc: 0.9932\n",
            "Epoch 351/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0621 - val_acc: 0.9932\n",
            "Epoch 352/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0622 - val_acc: 0.9932\n",
            "Epoch 353/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0622 - val_acc: 0.9932\n",
            "Epoch 354/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0622 - val_acc: 0.9932\n",
            "Epoch 355/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0622 - val_acc: 0.9932\n",
            "Epoch 356/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0623 - val_acc: 0.9932\n",
            "Epoch 357/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0623 - val_acc: 0.9932\n",
            "Epoch 358/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0623 - val_acc: 0.9932\n",
            "Epoch 359/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0623 - val_acc: 0.9932\n",
            "Epoch 360/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0623 - val_acc: 0.9932\n",
            "Epoch 361/500\n",
            "60000/60000 [==============================] - 3s 44us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0624 - val_acc: 0.9932\n",
            "Epoch 362/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0624 - val_acc: 0.9932\n",
            "Epoch 363/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0624 - val_acc: 0.9932\n",
            "Epoch 364/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0624 - val_acc: 0.9932\n",
            "Epoch 365/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0624 - val_acc: 0.9932\n",
            "Epoch 366/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0624 - val_acc: 0.9932\n",
            "Epoch 367/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0625 - val_acc: 0.9932\n",
            "Epoch 368/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0625 - val_acc: 0.9932\n",
            "Epoch 369/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0625 - val_acc: 0.9932\n",
            "Epoch 370/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0625 - val_acc: 0.9932\n",
            "Epoch 371/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0626 - val_acc: 0.9932\n",
            "Epoch 372/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0626 - val_acc: 0.9932\n",
            "Epoch 373/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0626 - val_acc: 0.9932\n",
            "Epoch 374/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0626 - val_acc: 0.9932\n",
            "Epoch 375/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0626 - val_acc: 0.9932\n",
            "Epoch 376/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0627 - val_acc: 0.9932\n",
            "Epoch 377/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0627 - val_acc: 0.9932\n",
            "Epoch 378/500\n",
            "60000/60000 [==============================] - 3s 44us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0627 - val_acc: 0.9932\n",
            "Epoch 379/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0627 - val_acc: 0.9932\n",
            "Epoch 380/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0627 - val_acc: 0.9932\n",
            "Epoch 381/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0627 - val_acc: 0.9932\n",
            "Epoch 382/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0627 - val_acc: 0.9932\n",
            "Epoch 383/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0627 - val_acc: 0.9932\n",
            "Epoch 384/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0628 - val_acc: 0.9932\n",
            "Epoch 385/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0628 - val_acc: 0.9932\n",
            "Epoch 386/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0628 - val_acc: 0.9932\n",
            "Epoch 387/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0628 - val_acc: 0.9932\n",
            "Epoch 388/500\n",
            "60000/60000 [==============================] - 3s 44us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0628 - val_acc: 0.9932\n",
            "Epoch 389/500\n",
            "60000/60000 [==============================] - 3s 44us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0628 - val_acc: 0.9932\n",
            "Epoch 390/500\n",
            "60000/60000 [==============================] - 3s 44us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0628 - val_acc: 0.9932\n",
            "Epoch 391/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0628 - val_acc: 0.9932\n",
            "Epoch 392/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0628 - val_acc: 0.9932\n",
            "Epoch 393/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0628 - val_acc: 0.9932\n",
            "Epoch 394/500\n",
            "60000/60000 [==============================] - 3s 44us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0628 - val_acc: 0.9932\n",
            "Epoch 395/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0629 - val_acc: 0.9932\n",
            "Epoch 396/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0629 - val_acc: 0.9932\n",
            "Epoch 397/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0629 - val_acc: 0.9932\n",
            "Epoch 398/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0629 - val_acc: 0.9932\n",
            "Epoch 399/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0629 - val_acc: 0.9932\n",
            "Epoch 400/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0629 - val_acc: 0.9932\n",
            "Epoch 401/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0629 - val_acc: 0.9932\n",
            "Epoch 402/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0629 - val_acc: 0.9932\n",
            "Epoch 403/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0629 - val_acc: 0.9932\n",
            "Epoch 404/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0629 - val_acc: 0.9932\n",
            "Epoch 405/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0630 - val_acc: 0.9932\n",
            "Epoch 406/500\n",
            "60000/60000 [==============================] - 3s 44us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0630 - val_acc: 0.9932\n",
            "Epoch 407/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0630 - val_acc: 0.9932\n",
            "Epoch 408/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0630 - val_acc: 0.9932\n",
            "Epoch 409/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0630 - val_acc: 0.9932\n",
            "Epoch 410/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0630 - val_acc: 0.9932\n",
            "Epoch 411/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0630 - val_acc: 0.9932\n",
            "Epoch 412/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0630 - val_acc: 0.9932\n",
            "Epoch 413/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0630 - val_acc: 0.9932\n",
            "Epoch 414/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0630 - val_acc: 0.9932\n",
            "Epoch 415/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0631 - val_acc: 0.9932\n",
            "Epoch 416/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0631 - val_acc: 0.9932\n",
            "Epoch 417/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0631 - val_acc: 0.9932\n",
            "Epoch 418/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0631 - val_acc: 0.9932\n",
            "Epoch 419/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0631 - val_acc: 0.9932\n",
            "Epoch 420/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0631 - val_acc: 0.9932\n",
            "Epoch 421/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0631 - val_acc: 0.9932\n",
            "Epoch 422/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0631 - val_acc: 0.9932\n",
            "Epoch 423/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0631 - val_acc: 0.9932\n",
            "Epoch 424/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0631 - val_acc: 0.9932\n",
            "Epoch 425/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0631 - val_acc: 0.9932\n",
            "Epoch 426/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0632 - val_acc: 0.9932\n",
            "Epoch 427/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0632 - val_acc: 0.9933\n",
            "Epoch 428/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0632 - val_acc: 0.9932\n",
            "Epoch 429/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0632 - val_acc: 0.9932\n",
            "Epoch 430/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0632 - val_acc: 0.9933\n",
            "Epoch 431/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0632 - val_acc: 0.9933\n",
            "Epoch 432/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0632 - val_acc: 0.9933\n",
            "Epoch 433/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0632 - val_acc: 0.9933\n",
            "Epoch 434/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0632 - val_acc: 0.9933\n",
            "Epoch 435/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0632 - val_acc: 0.9933\n",
            "Epoch 436/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0632 - val_acc: 0.9933\n",
            "Epoch 437/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0632 - val_acc: 0.9933\n",
            "Epoch 438/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0632 - val_acc: 0.9933\n",
            "Epoch 439/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0632 - val_acc: 0.9933\n",
            "Epoch 440/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0632 - val_acc: 0.9933\n",
            "Epoch 441/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0632 - val_acc: 0.9933\n",
            "Epoch 442/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0632 - val_acc: 0.9933\n",
            "Epoch 443/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0632 - val_acc: 0.9933\n",
            "Epoch 444/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0632 - val_acc: 0.9933\n",
            "Epoch 445/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0633 - val_acc: 0.9933\n",
            "Epoch 446/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0633 - val_acc: 0.9933\n",
            "Epoch 447/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0633 - val_acc: 0.9933\n",
            "Epoch 448/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0633 - val_acc: 0.9933\n",
            "Epoch 449/500\n",
            "60000/60000 [==============================] - 3s 44us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0633 - val_acc: 0.9933\n",
            "Epoch 450/500\n",
            "60000/60000 [==============================] - 3s 44us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0633 - val_acc: 0.9933\n",
            "Epoch 451/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0633 - val_acc: 0.9933\n",
            "Epoch 452/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0633 - val_acc: 0.9933\n",
            "Epoch 453/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0633 - val_acc: 0.9933\n",
            "Epoch 454/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0633 - val_acc: 0.9933\n",
            "Epoch 455/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0633 - val_acc: 0.9933\n",
            "Epoch 456/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0633 - val_acc: 0.9933\n",
            "Epoch 457/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0633 - val_acc: 0.9933\n",
            "Epoch 458/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0633 - val_acc: 0.9933\n",
            "Epoch 459/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0633 - val_acc: 0.9933\n",
            "Epoch 460/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0633 - val_acc: 0.9933\n",
            "Epoch 461/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0633 - val_acc: 0.9933\n",
            "Epoch 462/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0633 - val_acc: 0.9933\n",
            "Epoch 463/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0633 - val_acc: 0.9933\n",
            "Epoch 464/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0633 - val_acc: 0.9933\n",
            "Epoch 465/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0633 - val_acc: 0.9933\n",
            "Epoch 466/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0633 - val_acc: 0.9933\n",
            "Epoch 467/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0633 - val_acc: 0.9933\n",
            "Epoch 468/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0633 - val_acc: 0.9933\n",
            "Epoch 469/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0633 - val_acc: 0.9933\n",
            "Epoch 470/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0633 - val_acc: 0.9933\n",
            "Epoch 471/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0633 - val_acc: 0.9933\n",
            "Epoch 472/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0633 - val_acc: 0.9933\n",
            "Epoch 473/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0633 - val_acc: 0.9933\n",
            "Epoch 474/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0633 - val_acc: 0.9933\n",
            "Epoch 475/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0633 - val_acc: 0.9933\n",
            "Epoch 476/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0633 - val_acc: 0.9933\n",
            "Epoch 477/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0633 - val_acc: 0.9933\n",
            "Epoch 478/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0633 - val_acc: 0.9933\n",
            "Epoch 479/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0633 - val_acc: 0.9933\n",
            "Epoch 480/500\n",
            "60000/60000 [==============================] - 3s 44us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0633 - val_acc: 0.9933\n",
            "Epoch 481/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0633 - val_acc: 0.9933\n",
            "Epoch 482/500\n",
            "60000/60000 [==============================] - 3s 44us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0633 - val_acc: 0.9933\n",
            "Epoch 483/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0633 - val_acc: 0.9933\n",
            "Epoch 484/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0633 - val_acc: 0.9933\n",
            "Epoch 485/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0633 - val_acc: 0.9933\n",
            "Epoch 486/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0633 - val_acc: 0.9933\n",
            "Epoch 487/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0633 - val_acc: 0.9933\n",
            "Epoch 488/500\n",
            "60000/60000 [==============================] - 3s 44us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0633 - val_acc: 0.9933\n",
            "Epoch 489/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0633 - val_acc: 0.9933\n",
            "Epoch 490/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0633 - val_acc: 0.9933\n",
            "Epoch 491/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0633 - val_acc: 0.9933\n",
            "Epoch 492/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0633 - val_acc: 0.9933\n",
            "Epoch 493/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0633 - val_acc: 0.9933\n",
            "Epoch 494/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0633 - val_acc: 0.9933\n",
            "Epoch 495/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0633 - val_acc: 0.9933\n",
            "Epoch 496/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0633 - val_acc: 0.9933\n",
            "Epoch 497/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0633 - val_acc: 0.9933\n",
            "Epoch 498/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0633 - val_acc: 0.9933\n",
            "Epoch 499/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0633 - val_acc: 0.9933\n",
            "Epoch 500/500\n",
            "60000/60000 [==============================] - 3s 43us/step - loss: 2.6875e-04 - acc: 1.0000 - val_loss: 0.0633 - val_acc: 0.9933\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3e1024da20>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtsH-lLk-eLb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "score = model.evaluate(X_test, Y_test, verbose=0)         # Test the trained model on test data and print the results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkX8JMv79q9r",
        "colab_type": "code",
        "outputId": "f31d09af-1ae9-407f-8c11-069314af5466",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(score)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.06328659316992874, 0.9933]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCWoJkwE9suh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = model.predict(X_test)                           # Apply the model on test data to generate classification outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ym7iCFBm9uBs",
        "colab_type": "code",
        "outputId": "bfb40be7-2eb1-4fd9-c388-4f551591efed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "print(y_pred[:9])                                        # Print predicted and ground truth outputs\n",
        "print(y_test[:9])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.27679267e-36 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 1.03820820e-34 0.00000000e+00 1.06239795e-36\n",
            "  2.41566233e-37 0.00000000e+00]\n",
            " [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.11893159e-28 0.00000000e+00\n",
            "  3.85055689e-36 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 1.16090538e-23]\n",
            " [0.00000000e+00 1.00000000e+00 1.55011258e-38 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.45051687e-33\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  2.45268233e-38 1.25620756e-35]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  3.52212661e-35 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  4.50154527e-31 1.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 1.00000000e+00 1.21476304e-16 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]]\n",
            "[7 2 1 0 4 1 4 9 5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CT--y98_dr2T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "layer_dict = dict([(layer.name, layer) for layer in model.layers])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xo6ZVx2CKQ6p",
        "colab_type": "text"
      },
      "source": [
        "Display one of the input image and along with ouputs of specified layer channel to compare them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GY4Upv4dsUR",
        "colab_type": "code",
        "outputId": "06e2c6cf-7e4d-4204-ca1c-d0ce9d8c46d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 741
        }
      },
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from keras import backend as K\n",
        "%matplotlib inline\n",
        "# util function to convert a tensor into a valid image\n",
        "def deprocess_image(x):\n",
        "    # normalize tensor: center on 0., ensure std is 0.1\n",
        "    x -= x.mean()\n",
        "    x /= (x.std() + 1e-5)\n",
        "    x *= 0.1\n",
        "\n",
        "    # clip to [0, 1]\n",
        "    x += 0.5\n",
        "    x = np.clip(x, 0, 1)\n",
        "\n",
        "    # convert to RGB array\n",
        "    x *= 255\n",
        "    #x = x.transpose((1, 2, 0))\n",
        "    x = np.clip(x, 0, 255).astype('uint8')\n",
        "    return x\n",
        "\n",
        "def vis_img_in_filter(img = np.array(X_train[2]).reshape((1, 28, 28, 1)).astype(np.float64), \n",
        "                      layer_name = 'conv2d_14'):\n",
        "    layer_output = layer_dict[layer_name].output\n",
        "    img_ascs = list()\n",
        "    for filter_index in range(layer_output.shape[3]):\n",
        "        # build a loss function that maximizes the activation\n",
        "        # of the nth filter of the layer considered\n",
        "        loss = K.mean(layer_output[:, :, :, filter_index])\n",
        "\n",
        "        # compute the gradient of the input picture wrt this loss\n",
        "        grads = K.gradients(loss, model.input)[0]\n",
        "\n",
        "        # normalization trick: we normalize the gradient\n",
        "        grads /= (K.sqrt(K.mean(K.square(grads))) + 1e-5)\n",
        "\n",
        "        # this function returns the loss and grads given the input picture\n",
        "        iterate = K.function([model.input], [loss, grads])\n",
        "\n",
        "        # step size for gradient ascent\n",
        "        step = 5.\n",
        "\n",
        "        img_asc = np.array(img)\n",
        "        # run gradient ascent for 20 steps\n",
        "        for i in range(20):\n",
        "            loss_value, grads_value = iterate([img_asc])\n",
        "            img_asc += grads_value * step\n",
        "\n",
        "        img_asc = img_asc[0]\n",
        "        img_ascs.append(deprocess_image(img_asc).reshape((28, 28)))\n",
        "        \n",
        "    if layer_output.shape[3] >= 35:\n",
        "        plot_x, plot_y = 6, 6\n",
        "    elif layer_output.shape[3] >= 23:\n",
        "        plot_x, plot_y = 4, 6\n",
        "    elif layer_output.shape[3] >= 11:\n",
        "        plot_x, plot_y = 2, 6\n",
        "    else:\n",
        "        plot_x, plot_y = 1, 2\n",
        "    fig, ax = plt.subplots(plot_x, plot_y, figsize = (12, 12))\n",
        "    ax[0, 0].imshow(img.reshape((28, 28)), cmap = 'gray')\n",
        "    ax[0, 0].set_title('Input image')\n",
        "    fig.suptitle('Input image and %s filters' % (layer_name,))\n",
        "    fig.tight_layout(pad = 0.3, rect = [0, 0, 0.9, 0.9])\n",
        "    for (x, y) in [(i, j) for i in range(plot_x) for j in range(plot_y)]:\n",
        "        if x == 0 and y == 0:\n",
        "            continue\n",
        "        ax[x, y].imshow(img_ascs[x * plot_y + y - 1], cmap = 'gray')\n",
        "        ax[x, y].set_title('filter %d' % (x * plot_y + y - 1))\n",
        "\n",
        "vis_img_in_filter(layer_name = 'conv2d_11')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAwoAAALUCAYAAACre8XKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAICe/x98fP2JxYlGfwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 864x864 with 12 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEnFcoIYnwv3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}