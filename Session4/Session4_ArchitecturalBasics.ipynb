{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Session4_ArchitecturalBasics.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GauravPatel89/EVA-Track3-Assignments/blob/master/Session4/Session4_ArchitecturalBasics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNyZv-Ec52ot",
        "colab_type": "text"
      },
      "source": [
        "# **Import Libraries and modules**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1l5dG2pl1rT",
        "colab_type": "text"
      },
      "source": [
        "Following block installs keras in current runtime\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3m3w1Cw49Zkt",
        "colab_type": "code",
        "outputId": "3f3ef9d0-8efb-47f3-9a62-ebb3312b9960",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# https://keras.io/\n",
        "# !pip install -q keras\n",
        "import keras"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mysmkrBjl8b0",
        "colab_type": "text"
      },
      "source": [
        "Import the different packages to use in our code.\n",
        "\n",
        "Last line imports the MNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eso6UHE080D4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Add\n",
        "from keras.layers import Convolution2D, MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint,Callback\n",
        "\n",
        "from keras.datasets import mnist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zByEi95J86RD",
        "colab_type": "text"
      },
      "source": [
        "### Load pre-shuffled MNIST data into train and test sets\n",
        "\n",
        "Load MNIST dataset\n",
        "\n",
        "X_train - input training data\n",
        "\n",
        "y_train - training outputs corresponding to X_train\n",
        "\n",
        "X_test - input testing data\n",
        "\n",
        "y_test - testing outputs corresponding to X_test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eRM0QWN83PV",
        "colab_type": "code",
        "outputId": "4eab4301-5dca-4b4a-986c-c33ae6319f28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0V7k0loolLn",
        "colab_type": "text"
      },
      "source": [
        "Print the size of X_train.\n",
        "\n",
        "-Size of training set is 60000 images of size 28x28\n",
        "\n",
        "-Show first input training image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a4Be72j8-ZC",
        "colab_type": "code",
        "outputId": "306f00d7-c5ce-4750-a8fa-50f1a31e396b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        }
      },
      "source": [
        "print (X_train.shape)\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.imshow(X_train[0])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f17b182bf28>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evenqcZ0owVA",
        "colab_type": "text"
      },
      "source": [
        "Change the size of X_train and X_test from 60000x28x28 to 60000x28x28x1\n",
        "We are adding one more level of dimension to the input. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkmprriw9AnZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], 28, 28,1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYf5dktQo0T3",
        "colab_type": "text"
      },
      "source": [
        "Right now dataset loaded is of type uint8. Each pixel is integer of size 8 bit. We first convert training data to float of size 32 bits.\n",
        "\n",
        "To normalize the image data we divide whole data by 255 i.e. highest 8bit value for a pixel. After the division all the pixel values are between 0.0 and 1.0 float32."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2m4YS4E9CRh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-LmKcdSo532",
        "colab_type": "text"
      },
      "source": [
        "Print first 10 output classes for training dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Mn0vAYD9DvB",
        "colab_type": "code",
        "outputId": "0fd12d68-6a26-418d-91fb-e28ea818edea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_train[:10]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XwhiD4So9Gs",
        "colab_type": "text"
      },
      "source": [
        "Convert training and test output from simple class number to 'one hot encoding'. In this encoding we have as many bits as the number of classes.We have binary '1' for bit number corresponding to the class of the input. For other classes we have '0'.\n",
        "\n",
        "eg. '5' == '0 0 0 0 1 0 0 0 0 0' ; '10' == '0 0 0 0 0 0 0 0 0 1'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG8JiXR39FHC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Convert 1-dimensional class arrays to 10-dimensional class matrices\n",
        "Y_train = np_utils.to_categorical(y_train, 10)\n",
        "Y_test = np_utils.to_categorical(y_test, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYlFRvKS9HMB",
        "colab_type": "code",
        "outputId": "9f36b547-2a49-47be-b300-5d222c725785",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "Y_train[:10]           # print the first 10 training outputs"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6F27EhE4hDrA",
        "colab_type": "text"
      },
      "source": [
        "# First Code \n",
        "This is a Vannila network. We will not use BatchNormalization, DropOut, LR scheduler etc.\n",
        "\n",
        "We will try to finalize our model architecture in this code i.e. number of kernels to use, position of transition layer etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "c9a2cd11-a3bc-4637-b1be-9629fd9434bd",
        "id": "aBXUZmF6JBIE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 729
        }
      },
      "source": [
        "model1 = Sequential()\n",
        "                                                                              # RF = 1x1    i/p = 28x28x1     \n",
        "model1.add(Convolution2D(8, 3, 3, activation='relu', input_shape=(28,28,1)))  # RF = 3x3    i/p = 28x28x1  kernel = (3x3x1)x8   o/p = 26x26x8\n",
        "model1.add(Convolution2D(12, 3, 3, activation='relu'))                        # RF = 5x5    i/p = 26x26x1  kernel = (3x3x8)x12  o/p = 24x24x12\n",
        "\n",
        "# 1x1 Conv\n",
        "model1.add(Convolution2D(8, 1, activation='relu'))                            # RF = 5x5    i/p = 24x24x12 kernel = (1x1x12)x8  o/p = 24x24x8\n",
        "# 2x2 MaxPooling\n",
        "model1.add(MaxPooling2D(pool_size=(2, 2)))                                    # RF = 10x10  i/p = 24x24x8  kernel = MaxPool2x2  o/p = 12x12x8\n",
        "\n",
        "model1.add(Convolution2D(12, 3, 3, activation='relu'))                        # RF = 12x12  i/p = 12x12x8  kernel = (3x3x8)x12  o/p = 10x10x12\n",
        "model1.add(Convolution2D(12, 3, 3, activation='relu'))                        # RF = 14x14  i/p = 10x10x12 kernel = (3x3x12)x12 o/p = 8x8x12\n",
        "model1.add(Convolution2D(12, 3, 3, activation='relu'))                        # RF = 16x16  i/p =  8x8x12  kernel = (3x3x12)x12 o/p = 6x6x12\n",
        "\n",
        "# 1x1 Conv\n",
        "model1.add(Convolution2D(12, 1, activation='relu'))                           # RF = 16x16  i/p = 6x6x12   kernel = (1x1x12)x12 o/p = 6x6x12\n",
        "model1.add(Convolution2D(10, 6, 6))                                           # RF = 21x21  i/p = 6x6x12   kernel = (6x6x12)x10 o/p = 1x1x10\n",
        "model1.add(Flatten())                                                         # flatten the 1x1x10 array into 10x1 array\n",
        "model1.add(Activation('softmax'))                                             # apply the softmax function to obtain classification weights\n",
        "\n",
        "model1.summary()   #Show model information"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_17 (Conv2D)           (None, 26, 26, 8)         80        \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 24, 24, 12)        876       \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 24, 24, 8)         104       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 12, 12, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 10, 10, 12)        876       \n",
            "_________________________________________________________________\n",
            "conv2d_21 (Conv2D)           (None, 8, 8, 12)          1308      \n",
            "_________________________________________________________________\n",
            "conv2d_22 (Conv2D)           (None, 6, 6, 12)          1308      \n",
            "_________________________________________________________________\n",
            "conv2d_23 (Conv2D)           (None, 6, 6, 12)          156       \n",
            "_________________________________________________________________\n",
            "conv2d_24 (Conv2D)           (None, 1, 1, 10)          4330      \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 9,038\n",
            "Trainable params: 9,038\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(8, (3, 3), activation=\"relu\", input_shape=(28, 28, 1...)`\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(12, (3, 3), activation=\"relu\")`\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(12, (3, 3), activation=\"relu\")`\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(12, (3, 3), activation=\"relu\")`\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(12, (3, 3), activation=\"relu\")`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (6, 6))`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zp6SuGrL9M3h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# compile the model by specifyinng the optimizer, loss function, and training metrics to be used\n",
        "model1.compile(loss='categorical_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics=['accuracy'])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xWoKhPY9Of5",
        "colab_type": "code",
        "outputId": "d4df6d8e-d2b9-49dd-f38e-ddad6d989634",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#start the model training with test sample validation after each epoch. Use batch size as 32 and number of epochs as 20\n",
        "\n",
        "model1.fit(X_train, Y_train,validation_data=(X_test, Y_test), batch_size=32, nb_epoch=30, verbose=1)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/30\n",
            "60000/60000 [==============================] - 22s 363us/step - loss: 0.2202 - acc: 0.9289 - val_loss: 0.0624 - val_acc: 0.9798\n",
            "Epoch 2/30\n",
            "60000/60000 [==============================] - 21s 352us/step - loss: 0.0702 - acc: 0.9792 - val_loss: 0.0522 - val_acc: 0.9842\n",
            "Epoch 3/30\n",
            "60000/60000 [==============================] - 21s 352us/step - loss: 0.0565 - acc: 0.9825 - val_loss: 0.0644 - val_acc: 0.9796\n",
            "Epoch 4/30\n",
            "60000/60000 [==============================] - 21s 353us/step - loss: 0.0471 - acc: 0.9857 - val_loss: 0.0430 - val_acc: 0.9867\n",
            "Epoch 5/30\n",
            "60000/60000 [==============================] - 21s 355us/step - loss: 0.0407 - acc: 0.9870 - val_loss: 0.0362 - val_acc: 0.9889\n",
            "Epoch 6/30\n",
            "60000/60000 [==============================] - 21s 352us/step - loss: 0.0376 - acc: 0.9881 - val_loss: 0.0374 - val_acc: 0.9886\n",
            "Epoch 7/30\n",
            "60000/60000 [==============================] - 21s 352us/step - loss: 0.0324 - acc: 0.9896 - val_loss: 0.0371 - val_acc: 0.9887\n",
            "Epoch 8/30\n",
            "60000/60000 [==============================] - 21s 354us/step - loss: 0.0299 - acc: 0.9904 - val_loss: 0.0433 - val_acc: 0.9872\n",
            "Epoch 9/30\n",
            "60000/60000 [==============================] - 21s 351us/step - loss: 0.0260 - acc: 0.9917 - val_loss: 0.0400 - val_acc: 0.9878\n",
            "Epoch 10/30\n",
            "60000/60000 [==============================] - 21s 352us/step - loss: 0.0250 - acc: 0.9922 - val_loss: 0.0398 - val_acc: 0.9885\n",
            "Epoch 11/30\n",
            "60000/60000 [==============================] - 21s 353us/step - loss: 0.0230 - acc: 0.9924 - val_loss: 0.0384 - val_acc: 0.9877\n",
            "Epoch 12/30\n",
            "60000/60000 [==============================] - 21s 352us/step - loss: 0.0209 - acc: 0.9932 - val_loss: 0.0466 - val_acc: 0.9864\n",
            "Epoch 13/30\n",
            "60000/60000 [==============================] - 21s 358us/step - loss: 0.0196 - acc: 0.9937 - val_loss: 0.0386 - val_acc: 0.9899\n",
            "Epoch 14/30\n",
            "60000/60000 [==============================] - 21s 354us/step - loss: 0.0182 - acc: 0.9940 - val_loss: 0.0352 - val_acc: 0.9892\n",
            "Epoch 15/30\n",
            "60000/60000 [==============================] - 21s 352us/step - loss: 0.0168 - acc: 0.9945 - val_loss: 0.0374 - val_acc: 0.9900\n",
            "Epoch 16/30\n",
            "60000/60000 [==============================] - 21s 351us/step - loss: 0.0175 - acc: 0.9942 - val_loss: 0.0346 - val_acc: 0.9907\n",
            "Epoch 17/30\n",
            "60000/60000 [==============================] - 21s 352us/step - loss: 0.0153 - acc: 0.9950 - val_loss: 0.0431 - val_acc: 0.9889\n",
            "Epoch 18/30\n",
            "60000/60000 [==============================] - 21s 352us/step - loss: 0.0146 - acc: 0.9949 - val_loss: 0.0451 - val_acc: 0.9889\n",
            "Epoch 19/30\n",
            "60000/60000 [==============================] - 21s 355us/step - loss: 0.0149 - acc: 0.9951 - val_loss: 0.0463 - val_acc: 0.9870\n",
            "Epoch 20/30\n",
            "60000/60000 [==============================] - 21s 352us/step - loss: 0.0132 - acc: 0.9959 - val_loss: 0.0490 - val_acc: 0.9868\n",
            "Epoch 21/30\n",
            "60000/60000 [==============================] - 21s 353us/step - loss: 0.0142 - acc: 0.9956 - val_loss: 0.0416 - val_acc: 0.9896\n",
            "Epoch 22/30\n",
            "60000/60000 [==============================] - 21s 355us/step - loss: 0.0131 - acc: 0.9959 - val_loss: 0.0414 - val_acc: 0.9898\n",
            "Epoch 23/30\n",
            "60000/60000 [==============================] - 21s 352us/step - loss: 0.0113 - acc: 0.9961 - val_loss: 0.0437 - val_acc: 0.9902\n",
            "Epoch 24/30\n",
            "60000/60000 [==============================] - 21s 352us/step - loss: 0.0118 - acc: 0.9960 - val_loss: 0.0443 - val_acc: 0.9896\n",
            "Epoch 25/30\n",
            "60000/60000 [==============================] - 21s 354us/step - loss: 0.0110 - acc: 0.9963 - val_loss: 0.0539 - val_acc: 0.9878\n",
            "Epoch 26/30\n",
            "60000/60000 [==============================] - 21s 353us/step - loss: 0.0107 - acc: 0.9965 - val_loss: 0.0421 - val_acc: 0.9901\n",
            "Epoch 27/30\n",
            "60000/60000 [==============================] - 21s 354us/step - loss: 0.0108 - acc: 0.9966 - val_loss: 0.0591 - val_acc: 0.9870\n",
            "Epoch 28/30\n",
            "60000/60000 [==============================] - 21s 357us/step - loss: 0.0110 - acc: 0.9966 - val_loss: 0.0375 - val_acc: 0.9910\n",
            "Epoch 29/30\n",
            "60000/60000 [==============================] - 21s 352us/step - loss: 0.0103 - acc: 0.9968 - val_loss: 0.0526 - val_acc: 0.9900\n",
            "Epoch 30/30\n",
            "60000/60000 [==============================] - 21s 352us/step - loss: 0.0103 - acc: 0.9968 - val_loss: 0.0493 - val_acc: 0.9884\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f17a088a8d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-GZaeV1QeO5",
        "colab_type": "text"
      },
      "source": [
        "### First Code Observations\n",
        "\n",
        "Network has 9038 parameters, well below target of 15k.\n",
        "\n",
        "If we look at the validation accuracy we are achieving\n",
        "Validation accuracy as high as 98.89% (Epoch 5) in first 10 epochs and this crosses 99% (99.10 Epoch 28) if we continue for 30 epochs.\n",
        "\n",
        "Thus we can say the model has potential. The accuracy is going to increase once we add BatchNorm, Dropout, LR. So we can proceed with this architecture.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lbSaOyhYQCi3"
      },
      "source": [
        "# Second Code \n",
        "\n",
        "In this iteration we will add **BatchNormalization** to **model1** after every convolution and see if it helps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "a33c1849-cb7b-4cd2-f849-ee8870a6ac69",
        "id": "_43zE0ttbST2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 903
        }
      },
      "source": [
        "model2 = Sequential()\n",
        "                                                                              # RF = 1x1    i/p = 28x28x1     \n",
        "model2.add(Convolution2D(8, 3, 3, activation='relu', input_shape=(28,28,1)))  # RF = 3x3    i/p = 28x28x1  kernel = (3x3x1)x8   o/p = 26x26x8\n",
        "model2.add(BatchNormalization())      # Batch Normalization\n",
        "\n",
        "\n",
        "model2.add(Convolution2D(12, 3, 3, activation='relu'))                        # RF = 5x5    i/p = 26x26x1  kernel = (3x3x8)x12  o/p = 24x24x12\n",
        "model2.add(BatchNormalization())      # Batch Normalization\n",
        "\n",
        "# 1x1 Conv\n",
        "model2.add(Convolution2D(8, 1, activation='relu'))                            # RF = 5x5    i/p = 24x24x12 kernel = (1x1x12)x8  o/p = 24x24x8\n",
        "# 2x2 MaxPooling\n",
        "model2.add(MaxPooling2D(pool_size=(2, 2)))                                    # RF = 10x10  i/p = 24x24x8  kernel = MaxPool2x2  o/p = 12x12x8\n",
        "\n",
        "model2.add(Convolution2D(12, 3, 3, activation='relu'))                        # RF = 12x12  i/p = 12x12x8  kernel = (3x3x8)x12  o/p = 10x10x12\n",
        "model2.add(BatchNormalization())      # Batch Normalization\n",
        "\n",
        "model2.add(Convolution2D(12, 3, 3, activation='relu'))                        # RF = 14x14  i/p = 10x10x12 kernel = (3x3x12)x12 o/p = 8x8x12\n",
        "model2.add(BatchNormalization())      # Batch Normalization\n",
        "\n",
        "model2.add(Convolution2D(12, 3, 3, activation='relu'))                        # RF = 16x16  i/p =  8x8x12  kernel = (3x3x12)x12 o/p = 6x6x12\n",
        "model2.add(BatchNormalization())      # Batch Normalization\n",
        "\n",
        "# 1x1 Conv\n",
        "model2.add(Convolution2D(12, 1, activation='relu'))                           # RF = 16x16  i/p = 6x6x12   kernel = (1x1x12)x12 o/p = 6x6x12\n",
        "model2.add(BatchNormalization())      # Batch Normalization\n",
        "\n",
        "model2.add(Convolution2D(10, 6, 6))                                           # RF = 21x21  i/p = 6x6x12   kernel = (6x6x12)x10 o/p = 1x1x10\n",
        "\n",
        "model2.add(Flatten())                                                         # flatten the 1x1x10 array into 10x1 array\n",
        "model2.add(Activation('softmax'))                                             # apply the softmax function to obtain classification weights\n",
        "\n",
        "model2.summary()   #Show model information"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(8, (3, 3), activation=\"relu\", input_shape=(28, 28, 1...)`\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(12, (3, 3), activation=\"relu\")`\n",
            "  import sys\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(12, (3, 3), activation=\"relu\")`\n",
            "  from ipykernel import kernelapp as app\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(12, (3, 3), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(12, (3, 3), activation=\"relu\")`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_33 (Conv2D)           (None, 26, 26, 8)         80        \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 26, 26, 8)         32        \n",
            "_________________________________________________________________\n",
            "conv2d_34 (Conv2D)           (None, 24, 24, 12)        876       \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 24, 24, 12)        48        \n",
            "_________________________________________________________________\n",
            "conv2d_35 (Conv2D)           (None, 24, 24, 8)         104       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 12, 12, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_36 (Conv2D)           (None, 10, 10, 12)        876       \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 10, 10, 12)        48        \n",
            "_________________________________________________________________\n",
            "conv2d_37 (Conv2D)           (None, 8, 8, 12)          1308      \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 8, 8, 12)          48        \n",
            "_________________________________________________________________\n",
            "conv2d_38 (Conv2D)           (None, 6, 6, 12)          1308      \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 6, 6, 12)          48        \n",
            "_________________________________________________________________\n",
            "conv2d_39 (Conv2D)           (None, 6, 6, 12)          156       \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 6, 6, 12)          48        \n",
            "_________________________________________________________________\n",
            "conv2d_40 (Conv2D)           (None, 1, 1, 10)          4330      \n",
            "_________________________________________________________________\n",
            "flatten_5 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 9,310\n",
            "Trainable params: 9,174\n",
            "Non-trainable params: 136\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:28: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (6, 6))`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "0d2f18fd-7489-4352-d3b2-5b4ecc6ddbb5",
        "id": "O2YYrDNMP-nT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 781
        }
      },
      "source": [
        "# compile the model by specifyinng the optimizer, loss function, and training metrics to be used\n",
        "model2.compile(loss='categorical_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics=['accuracy'])\n",
        "\n",
        "#start the model training with test sample validation after each epoch. Use batch size as 32 and number of epochs as 20\n",
        "\n",
        "model2.fit(X_train, Y_train,validation_data=(X_test, Y_test), batch_size=32, nb_epoch=20, verbose=1)\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  import sys\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 35s 590us/step - loss: 0.1863 - acc: 0.9426 - val_loss: 0.0793 - val_acc: 0.9751\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 33s 557us/step - loss: 0.0610 - acc: 0.9807 - val_loss: 0.0582 - val_acc: 0.9812\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 33s 556us/step - loss: 0.0478 - acc: 0.9851 - val_loss: 0.0425 - val_acc: 0.9860\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 33s 556us/step - loss: 0.0391 - acc: 0.9874 - val_loss: 0.0495 - val_acc: 0.9832\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 33s 555us/step - loss: 0.0336 - acc: 0.9890 - val_loss: 0.0421 - val_acc: 0.9876\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 33s 555us/step - loss: 0.0302 - acc: 0.9899 - val_loss: 0.0486 - val_acc: 0.9849\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 33s 551us/step - loss: 0.0260 - acc: 0.9914 - val_loss: 0.0335 - val_acc: 0.9897\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 33s 554us/step - loss: 0.0237 - acc: 0.9922 - val_loss: 0.0558 - val_acc: 0.9833\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 33s 556us/step - loss: 0.0216 - acc: 0.9931 - val_loss: 0.0389 - val_acc: 0.9878\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 34s 564us/step - loss: 0.0195 - acc: 0.9931 - val_loss: 0.0452 - val_acc: 0.9876\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 33s 555us/step - loss: 0.0169 - acc: 0.9944 - val_loss: 0.0415 - val_acc: 0.9874\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 33s 555us/step - loss: 0.0169 - acc: 0.9944 - val_loss: 0.0394 - val_acc: 0.9893\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 34s 560us/step - loss: 0.0150 - acc: 0.9950 - val_loss: 0.0372 - val_acc: 0.9898\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 33s 556us/step - loss: 0.0144 - acc: 0.9950 - val_loss: 0.0413 - val_acc: 0.9887\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 33s 556us/step - loss: 0.0127 - acc: 0.9958 - val_loss: 0.0449 - val_acc: 0.9875\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 33s 557us/step - loss: 0.0130 - acc: 0.9956 - val_loss: 0.0366 - val_acc: 0.9896\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 33s 556us/step - loss: 0.0112 - acc: 0.9961 - val_loss: 0.0466 - val_acc: 0.9888\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 33s 558us/step - loss: 0.0113 - acc: 0.9963 - val_loss: 0.0437 - val_acc: 0.9898\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 34s 562us/step - loss: 0.0113 - acc: 0.9964 - val_loss: 0.0437 - val_acc: 0.9898\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 33s 557us/step - loss: 0.0094 - acc: 0.9965 - val_loss: 0.0447 - val_acc: 0.9897\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1761730710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YUf6GFX8cpDu"
      },
      "source": [
        "### Second Code Observations\n",
        "\n",
        "BatchNormalization has increased trainable parameters from 9038 to 9174 and also added 136 nontrainable parameters.\n",
        "\n",
        "If we look at the validation accuracy we are achieving\n",
        "Validation accuracy as high as 98.98% (Epoch 19) in 20 epochs\n",
        "\n",
        "If we look at the training accuracy, we are hitting high of 99.65% . We can see there is considerable gap between training and validation accuracy (~ 0.67%). This may be due to overfitting because model is performing well on training set but not that well with test set. We can remedy this by introdicing DropOut.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "G-CB39qycrqN"
      },
      "source": [
        "# Third Code \n",
        "\n",
        "\n",
        "In this iteration we will add **DropOut** to **model2** after every convolution and see if it helps. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "968e7ed9-423a-47ec-970b-758f60f239d4",
        "id": "eU0INo2Ucrqq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model3 = Sequential()\n",
        "                                                                              # RF = 1x1    i/p = 28x28x1     \n",
        "model3.add(Convolution2D(8, 3, 3, activation='relu', input_shape=(28,28,1)))  # RF = 3x3    i/p = 28x28x1  kernel = (3x3x1)x8   o/p = 26x26x8\n",
        "model3.add(BatchNormalization())      # Batch Normalization\n",
        "model3.add(Dropout(0.05))             # DropOut 5%\n",
        "\n",
        "model3.add(Convolution2D(12, 3, 3, activation='relu'))                        # RF = 5x5    i/p = 26x26x1  kernel = (3x3x8)x12  o/p = 24x24x12\n",
        "model3.add(BatchNormalization())      # Batch Normalization\n",
        "model3.add(Dropout(0.08))             # DropOut 8%\n",
        "\n",
        "# 1x1 Conv\n",
        "model3.add(Convolution2D(8, 1, activation='relu'))                            # RF = 5x5    i/p = 24x24x12 kernel = (1x1x12)x8  o/p = 24x24x8\n",
        "# 2x2 MaxPooling\n",
        "model3.add(MaxPooling2D(pool_size=(2, 2)))                                    # RF = 10x10  i/p = 24x24x8  kernel = MaxPool2x2  o/p = 12x12x8\n",
        "\n",
        "model3.add(Convolution2D(12, 3, 3, activation='relu'))                        # RF = 12x12  i/p = 12x12x8  kernel = (3x3x8)x12  o/p = 10x10x12\n",
        "model3.add(BatchNormalization())      # Batch Normalization\n",
        "model3.add(Dropout(0.08))             # DropOut 8%\n",
        "\n",
        "model3.add(Convolution2D(12, 3, 3, activation='relu'))                        # RF = 14x14  i/p = 10x10x12 kernel = (3x3x12)x12 o/p = 8x8x12\n",
        "model3.add(BatchNormalization())      # Batch Normalization\n",
        "model3.add(Dropout(0.08))             # DropOut 8%\n",
        "\n",
        "model3.add(Convolution2D(12, 3, 3, activation='relu'))                        # RF = 16x16  i/p =  8x8x12  kernel = (3x3x12)x12 o/p = 6x6x12\n",
        "model3.add(BatchNormalization())      # Batch Normalization\n",
        "model3.add(Dropout(0.08))             # DropOut 8%\n",
        "\n",
        "# 1x1 Conv\n",
        "model3.add(Convolution2D(12, 1, activation='relu'))                           # RF = 16x16  i/p = 6x6x12   kernel = (1x1x12)x12 o/p = 6x6x12\n",
        "model3.add(BatchNormalization())      # Batch Normalization\n",
        "model3.add(Dropout(0.08))             # DropOut 8%\n",
        "\n",
        "model3.add(Convolution2D(10, 6, 6))                                           # RF = 21x21  i/p = 6x6x12   kernel = (6x6x12)x10 o/p = 1x1x10\n",
        "\n",
        "model3.add(Flatten())                                                         # flatten the 1x1x10 array into 10x1 array\n",
        "model3.add(Activation('softmax'))                                             # apply the softmax function to obtain classification weights\n",
        "\n",
        "model3.summary()   #Show model information"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(8, (3, 3), activation=\"relu\", input_shape=(28, 28, 1...)`\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(12, (3, 3), activation=\"relu\")`\n",
            "  import sys\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(12, (3, 3), activation=\"relu\")`\n",
            "  app.launch_new_instance()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(12, (3, 3), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(12, (3, 3), activation=\"relu\")`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_41 (Conv2D)           (None, 26, 26, 8)         80        \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 26, 26, 8)         32        \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 26, 26, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_42 (Conv2D)           (None, 24, 24, 12)        876       \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 24, 24, 12)        48        \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 24, 24, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_43 (Conv2D)           (None, 24, 24, 8)         104       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 12, 12, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_44 (Conv2D)           (None, 10, 10, 12)        876       \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 10, 10, 12)        48        \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 10, 10, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_45 (Conv2D)           (None, 8, 8, 12)          1308      \n",
            "_________________________________________________________________\n",
            "batch_normalization_15 (Batc (None, 8, 8, 12)          48        \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 8, 8, 12)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_46 (Conv2D)           (None, 6, 6, 12)          1308      \n",
            "_________________________________________________________________\n",
            "batch_normalization_16 (Batc (None, 6, 6, 12)          48        \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 6, 6, 12)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_47 (Conv2D)           (None, 6, 6, 12)          156       \n",
            "_________________________________________________________________\n",
            "batch_normalization_17 (Batc (None, 6, 6, 12)          48        \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 6, 6, 12)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_48 (Conv2D)           (None, 1, 1, 10)          4330      \n",
            "_________________________________________________________________\n",
            "flatten_6 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 9,310\n",
            "Trainable params: 9,174\n",
            "Non-trainable params: 136\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (6, 6))`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "9fe3a4ff-215f-40b4-a2ec-748aec034bba",
        "id": "mj2OR4augNRH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 781
        }
      },
      "source": [
        "# compile the model by specifyinng the optimizer, loss function, and training metrics to be used\n",
        "model3.compile(loss='categorical_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics=['accuracy'])\n",
        "\n",
        "#start the model training with test sample validation after each epoch. Use batch size as 32 and number of epochs as 20\n",
        "\n",
        "model3.fit(X_train, Y_train,validation_data=(X_test, Y_test), batch_size=32, nb_epoch=20, verbose=1)\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  import sys\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 39s 653us/step - loss: 0.2205 - acc: 0.9299 - val_loss: 0.0616 - val_acc: 0.9800\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 37s 622us/step - loss: 0.0760 - acc: 0.9762 - val_loss: 0.0516 - val_acc: 0.9828\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 37s 621us/step - loss: 0.0618 - acc: 0.9800 - val_loss: 0.0433 - val_acc: 0.9859\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 37s 618us/step - loss: 0.0508 - acc: 0.9841 - val_loss: 0.0345 - val_acc: 0.9885\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 37s 617us/step - loss: 0.0494 - acc: 0.9848 - val_loss: 0.0446 - val_acc: 0.9850\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 37s 619us/step - loss: 0.0431 - acc: 0.9856 - val_loss: 0.0375 - val_acc: 0.9885\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 37s 617us/step - loss: 0.0405 - acc: 0.9863 - val_loss: 0.0324 - val_acc: 0.9895\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 37s 622us/step - loss: 0.0389 - acc: 0.9878 - val_loss: 0.0333 - val_acc: 0.9885\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 37s 617us/step - loss: 0.0383 - acc: 0.9876 - val_loss: 0.0300 - val_acc: 0.9899\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 37s 621us/step - loss: 0.0361 - acc: 0.9886 - val_loss: 0.0287 - val_acc: 0.9910\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 37s 616us/step - loss: 0.0332 - acc: 0.9893 - val_loss: 0.0301 - val_acc: 0.9893\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 37s 618us/step - loss: 0.0339 - acc: 0.9892 - val_loss: 0.0342 - val_acc: 0.9888\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 37s 618us/step - loss: 0.0314 - acc: 0.9898 - val_loss: 0.0239 - val_acc: 0.9913\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 37s 618us/step - loss: 0.0306 - acc: 0.9902 - val_loss: 0.0247 - val_acc: 0.9919\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 37s 616us/step - loss: 0.0306 - acc: 0.9898 - val_loss: 0.0255 - val_acc: 0.9922\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 38s 625us/step - loss: 0.0290 - acc: 0.9906 - val_loss: 0.0266 - val_acc: 0.9918\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 37s 620us/step - loss: 0.0278 - acc: 0.9910 - val_loss: 0.0240 - val_acc: 0.9925\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 37s 621us/step - loss: 0.0272 - acc: 0.9912 - val_loss: 0.0234 - val_acc: 0.9926\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 37s 620us/step - loss: 0.0269 - acc: 0.9915 - val_loss: 0.0282 - val_acc: 0.9916\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 37s 621us/step - loss: 0.0266 - acc: 0.9915 - val_loss: 0.0297 - val_acc: 0.9907\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1760b51e80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fmy_VIUjh01",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "outputId": "47a36314-32ae-4c64-9deb-258eac962424"
      },
      "source": [
        "model3.fit(X_train, Y_train,validation_data=(X_test, Y_test), batch_size=64, nb_epoch=10, verbose=1)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "  256/60000 [..............................] - ETA: 32s - loss: 0.0846 - acc: 0.9844 "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "60000/60000 [==============================] - 20s 339us/step - loss: 0.0202 - acc: 0.9935 - val_loss: 0.0229 - val_acc: 0.9926\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 20s 334us/step - loss: 0.0184 - acc: 0.9941 - val_loss: 0.0240 - val_acc: 0.9924\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 20s 334us/step - loss: 0.0209 - acc: 0.9933 - val_loss: 0.0277 - val_acc: 0.9910\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 20s 334us/step - loss: 0.0198 - acc: 0.9937 - val_loss: 0.0226 - val_acc: 0.9931\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 20s 336us/step - loss: 0.0195 - acc: 0.9933 - val_loss: 0.0238 - val_acc: 0.9928\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 20s 335us/step - loss: 0.0197 - acc: 0.9935 - val_loss: 0.0265 - val_acc: 0.9922\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 20s 335us/step - loss: 0.0189 - acc: 0.9936 - val_loss: 0.0244 - val_acc: 0.9928\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 20s 335us/step - loss: 0.0187 - acc: 0.9937 - val_loss: 0.0240 - val_acc: 0.9927\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 20s 334us/step - loss: 0.0186 - acc: 0.9937 - val_loss: 0.0260 - val_acc: 0.9915\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 20s 334us/step - loss: 0.0182 - acc: 0.9942 - val_loss: 0.0215 - val_acc: 0.9929\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1760bbf4e0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "248144b6-87c0-4019-8f2a-9c6089f0d4bb",
        "id": "sxSnyZnKkeZR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        }
      },
      "source": [
        "model3.fit(X_train, Y_train,validation_data=(X_test, Y_test), batch_size=64, nb_epoch=10, verbose=1)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "  448/60000 [..............................] - ETA: 20s - loss: 0.0261 - acc: 0.9933"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "60000/60000 [==============================] - 20s 335us/step - loss: 0.0180 - acc: 0.9942 - val_loss: 0.0234 - val_acc: 0.9925\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 20s 335us/step - loss: 0.0182 - acc: 0.9939 - val_loss: 0.0256 - val_acc: 0.9923\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 20s 335us/step - loss: 0.0182 - acc: 0.9939 - val_loss: 0.0247 - val_acc: 0.9925\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 20s 336us/step - loss: 0.0173 - acc: 0.9944 - val_loss: 0.0232 - val_acc: 0.9928\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 20s 335us/step - loss: 0.0176 - acc: 0.9939 - val_loss: 0.0219 - val_acc: 0.9925\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 20s 339us/step - loss: 0.0169 - acc: 0.9942 - val_loss: 0.0231 - val_acc: 0.9916\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 20s 335us/step - loss: 0.0160 - acc: 0.9944 - val_loss: 0.0250 - val_acc: 0.9919\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 20s 334us/step - loss: 0.0161 - acc: 0.9946 - val_loss: 0.0227 - val_acc: 0.9934\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 20s 336us/step - loss: 0.0174 - acc: 0.9939 - val_loss: 0.0236 - val_acc: 0.9927\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 20s 334us/step - loss: 0.0175 - acc: 0.9943 - val_loss: 0.0278 - val_acc: 0.9920\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f175f60c2b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8PQYXr1tgNRm"
      },
      "source": [
        "### Third Code Observations\n",
        "\n",
        "The number of trainable and non-trainable parameters is unchanged from the last model because DropOut doesn't require any parameters\n",
        "\n",
        "\n",
        "We trained this network for 20 epochs with batch size of 32 followed by 20 epochs with batch size of 64.\n",
        "\n",
        "\n",
        "If we look at the validation accuracy, during the first 20 epochs validation accuracy is steadily increasing for around 12-13 epochs but then for rest of the epochs it seems as if model is stuck. Accuracy is increasing slighly or reducing a bit but more or less stuck. This could be because we are using constant learning rate (default value of ADAM).\n",
        "\n",
        "We can remedy this by varying the learning rate with epoch. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xLPPXSlfgNRp"
      },
      "source": [
        "# Fourth Code (Final)\n",
        "\n",
        "\n",
        "In this iteration model is same as previous iteration (**model3**) We will be introducing Learning rate scheduler. Instead of using fixed learning rate the scheduler will vary the LR based on Epoch number. It will go on reducing the LR. We will also be using larger batch size for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "03a0040f-b31a-433b-e547-7f76dbb9efaf",
        "id": "O4Dmc3Qdi-Zz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model4 = Sequential()\n",
        "                                                                              # RF = 1x1    i/p = 28x28x1     \n",
        "model4.add(Convolution2D(8, 3, 3, activation='relu', input_shape=(28,28,1)))  # RF = 3x3    i/p = 28x28x1  kernel = (3x3x1)x8   o/p = 26x26x8\n",
        "model4.add(BatchNormalization())      # Batch Normalization\n",
        "model4.add(Dropout(0.05))             # DropOut 5%\n",
        "\n",
        "model4.add(Convolution2D(12, 3, 3, activation='relu'))                        # RF = 5x5    i/p = 26x26x1  kernel = (3x3x8)x12  o/p = 24x24x12\n",
        "model4.add(BatchNormalization())      # Batch Normalization\n",
        "model4.add(Dropout(0.08))             # DropOut 8%\n",
        "\n",
        "# 1x1 Conv\n",
        "model4.add(Convolution2D(8, 1, activation='relu'))                            # RF = 5x5    i/p = 24x24x12 kernel = (1x1x12)x8  o/p = 24x24x8\n",
        "# 2x2 MaxPooling\n",
        "model4.add(MaxPooling2D(pool_size=(2, 2)))                                    # RF = 10x10  i/p = 24x24x8  kernel = MaxPool2x2  o/p = 12x12x8\n",
        "\n",
        "model4.add(Convolution2D(12, 3, 3, activation='relu'))                        # RF = 12x12  i/p = 12x12x8  kernel = (3x3x8)x12  o/p = 10x10x12\n",
        "model4.add(BatchNormalization())      # Batch Normalization\n",
        "model4.add(Dropout(0.08))             # DropOut 8%\n",
        "\n",
        "model4.add(Convolution2D(12, 3, 3, activation='relu'))                        # RF = 14x14  i/p = 10x10x12 kernel = (3x3x12)x12 o/p = 8x8x12\n",
        "model4.add(BatchNormalization())      # Batch Normalization\n",
        "model4.add(Dropout(0.08))             # DropOut 8%\n",
        "\n",
        "model4.add(Convolution2D(12, 3, 3, activation='relu'))                        # RF = 16x16  i/p =  8x8x12  kernel = (3x3x12)x12 o/p = 6x6x12\n",
        "model4.add(BatchNormalization())      # Batch Normalization\n",
        "model4.add(Dropout(0.08))             # DropOut 8%\n",
        "\n",
        "# 1x1 Conv\n",
        "model4.add(Convolution2D(12, 1, activation='relu'))                           # RF = 16x16  i/p = 6x6x12   kernel = (1x1x12)x12 o/p = 6x6x12\n",
        "model4.add(BatchNormalization())      # Batch Normalization\n",
        "model4.add(Dropout(0.08))             # DropOut 8%\n",
        "\n",
        "model4.add(Convolution2D(10, 6, 6))                                           # RF = 21x21  i/p = 6x6x12   kernel = (6x6x12)x10 o/p = 1x1x10\n",
        "\n",
        "model4.add(Flatten())                                                         # flatten the 1x1x10 array into 10x1 array\n",
        "model4.add(Activation('softmax'))                                             # apply the softmax function to obtain classification weights\n",
        "\n",
        "model4.summary()   #Show model information"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(8, (3, 3), activation=\"relu\", input_shape=(28, 28, 1...)`\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(12, (3, 3), activation=\"relu\")`\n",
            "  import sys\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(12, (3, 3), activation=\"relu\")`\n",
            "  app.launch_new_instance()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(12, (3, 3), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(12, (3, 3), activation=\"relu\")`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_57 (Conv2D)           (None, 26, 26, 8)         80        \n",
            "_________________________________________________________________\n",
            "batch_normalization_24 (Batc (None, 26, 26, 8)         32        \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 26, 26, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_58 (Conv2D)           (None, 24, 24, 12)        876       \n",
            "_________________________________________________________________\n",
            "batch_normalization_25 (Batc (None, 24, 24, 12)        48        \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 24, 24, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_59 (Conv2D)           (None, 24, 24, 8)         104       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 12, 12, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_60 (Conv2D)           (None, 10, 10, 12)        876       \n",
            "_________________________________________________________________\n",
            "batch_normalization_26 (Batc (None, 10, 10, 12)        48        \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 10, 10, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_61 (Conv2D)           (None, 8, 8, 12)          1308      \n",
            "_________________________________________________________________\n",
            "batch_normalization_27 (Batc (None, 8, 8, 12)          48        \n",
            "_________________________________________________________________\n",
            "dropout_16 (Dropout)         (None, 8, 8, 12)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_62 (Conv2D)           (None, 6, 6, 12)          1308      \n",
            "_________________________________________________________________\n",
            "batch_normalization_28 (Batc (None, 6, 6, 12)          48        \n",
            "_________________________________________________________________\n",
            "dropout_17 (Dropout)         (None, 6, 6, 12)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_63 (Conv2D)           (None, 6, 6, 12)          156       \n",
            "_________________________________________________________________\n",
            "batch_normalization_29 (Batc (None, 6, 6, 12)          48        \n",
            "_________________________________________________________________\n",
            "dropout_18 (Dropout)         (None, 6, 6, 12)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_64 (Conv2D)           (None, 1, 1, 10)          4330      \n",
            "_________________________________________________________________\n",
            "flatten_8 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 9,310\n",
            "Trainable params: 9,174\n",
            "Non-trainable params: 136\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (6, 6))`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "88414553-5fb2-4033-c665-ae776ff35573",
        "id": "Fkr7JElm-sFd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "def scheduler(epoch, lr):\n",
        "  return round(0.009 * 1/(1 + 0.55 * epoch), 10)\n",
        "\n",
        "model4.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.009), metrics=['accuracy'])\n",
        "\n",
        "model4.fit(X_train, Y_train, batch_size=128, epochs=20, verbose=1, validation_data=(X_test, Y_test), callbacks=[LearningRateScheduler(scheduler, verbose=1)])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.009.\n",
            "60000/60000 [==============================] - 17s 283us/step - loss: 0.0516 - acc: 0.9837 - val_loss: 0.0345 - val_acc: 0.9883\n",
            "Epoch 2/20\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.0058064516.\n",
            "60000/60000 [==============================] - 12s 208us/step - loss: 0.0357 - acc: 0.9885 - val_loss: 0.0359 - val_acc: 0.9886\n",
            "Epoch 3/20\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.0042857143.\n",
            "60000/60000 [==============================] - 12s 206us/step - loss: 0.0291 - acc: 0.9905 - val_loss: 0.0276 - val_acc: 0.9913\n",
            "Epoch 4/20\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.0033962264.\n",
            "60000/60000 [==============================] - 13s 210us/step - loss: 0.0259 - acc: 0.9918 - val_loss: 0.0238 - val_acc: 0.9923\n",
            "Epoch 5/20\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.0028125.\n",
            "60000/60000 [==============================] - 12s 208us/step - loss: 0.0247 - acc: 0.9919 - val_loss: 0.0234 - val_acc: 0.9924\n",
            "Epoch 6/20\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.0024.\n",
            "60000/60000 [==============================] - 13s 209us/step - loss: 0.0217 - acc: 0.9925 - val_loss: 0.0253 - val_acc: 0.9923\n",
            "Epoch 7/20\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0020930233.\n",
            "60000/60000 [==============================] - 13s 210us/step - loss: 0.0218 - acc: 0.9926 - val_loss: 0.0219 - val_acc: 0.9932\n",
            "Epoch 8/20\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0018556701.\n",
            "60000/60000 [==============================] - 12s 207us/step - loss: 0.0193 - acc: 0.9939 - val_loss: 0.0225 - val_acc: 0.9933\n",
            "Epoch 9/20\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.0016666667.\n",
            "60000/60000 [==============================] - 13s 208us/step - loss: 0.0174 - acc: 0.9940 - val_loss: 0.0228 - val_acc: 0.9935\n",
            "Epoch 10/20\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.001512605.\n",
            "60000/60000 [==============================] - 13s 208us/step - loss: 0.0180 - acc: 0.9942 - val_loss: 0.0254 - val_acc: 0.9932\n",
            "Epoch 11/20\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.0013846154.\n",
            "60000/60000 [==============================] - 12s 208us/step - loss: 0.0172 - acc: 0.9942 - val_loss: 0.0222 - val_acc: 0.9938\n",
            "Epoch 12/20\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.0012765957.\n",
            "60000/60000 [==============================] - 13s 209us/step - loss: 0.0170 - acc: 0.9944 - val_loss: 0.0200 - val_acc: 0.9945\n",
            "Epoch 13/20\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.0011842105.\n",
            "60000/60000 [==============================] - 13s 209us/step - loss: 0.0150 - acc: 0.9953 - val_loss: 0.0225 - val_acc: 0.9939\n",
            "Epoch 14/20\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.0011042945.\n",
            "60000/60000 [==============================] - 13s 210us/step - loss: 0.0156 - acc: 0.9947 - val_loss: 0.0233 - val_acc: 0.9932\n",
            "Epoch 15/20\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.0010344828.\n",
            "60000/60000 [==============================] - 13s 210us/step - loss: 0.0147 - acc: 0.9953 - val_loss: 0.0224 - val_acc: 0.9927\n",
            "Epoch 16/20\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.000972973.\n",
            "60000/60000 [==============================] - 13s 209us/step - loss: 0.0136 - acc: 0.9956 - val_loss: 0.0219 - val_acc: 0.9935\n",
            "Epoch 17/20\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.0009183673.\n",
            "60000/60000 [==============================] - 12s 207us/step - loss: 0.0145 - acc: 0.9952 - val_loss: 0.0219 - val_acc: 0.9941\n",
            "Epoch 18/20\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.0008695652.\n",
            "60000/60000 [==============================] - 12s 208us/step - loss: 0.0139 - acc: 0.9955 - val_loss: 0.0229 - val_acc: 0.9935\n",
            "Epoch 19/20\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.0008256881.\n",
            "60000/60000 [==============================] - 12s 207us/step - loss: 0.0135 - acc: 0.9955 - val_loss: 0.0228 - val_acc: 0.9935\n",
            "Epoch 20/20\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.0007860262.\n",
            "60000/60000 [==============================] - 12s 207us/step - loss: 0.0128 - acc: 0.9957 - val_loss: 0.0204 - val_acc: 0.9933\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7c0123de48>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YpPag5C9nW5F"
      },
      "source": [
        "### Fourth Code Observations\n",
        "\n",
        "The number of trainable and non-trainable parameters is unchanged from the last model.\n",
        "\n",
        "We see that we are achieving validation accuracy of 99.45% in the 12th epoch. \n",
        "\n",
        "Thus our objective is achieved. \n",
        "99.45% accuracy with 9k parameters"
      ]
    }
  ]
}